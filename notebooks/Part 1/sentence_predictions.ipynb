{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# external libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from ast import literal_eval\n",
    "import time\n",
    "import sys \n",
    "from shutil import copyfile\n",
    "# tensorflow and keras\n",
    "import keras.optimizers\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Concatenate, Bidirectional, Reshape\n",
    "from keras.layers import GRU, CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.constraints import maxnorm\n",
    "from keras.regularizers import L1L2\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.backend import tile\n",
    "import keras.backend as K\n",
    "from keras.layers import Lambda\n",
    "# fix random seed for reproducibility - only works for CPU version of tensorflow\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = pd.read_csv('../../../../data/processed/tok_sentence_baby_reviews.csv')\n",
    "reviews_df = pd.read_csv('../../../../data/processed/tok_baby_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reviews_df.merge(sentences_df, on='uuid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Files read, converting tokens to lists.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFiles read, converting tokens to lists.\")\n",
    "for col in ['summary_tokens', 'review_tokens', 'sentence_tokens']:\n",
    "    df[col] = df[col].map(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>polarity</th>\n",
       "      <th>year</th>\n",
       "      <th>uuid</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>summary_tokens</th>\n",
       "      <th>summary_wc</th>\n",
       "      <th>review_wc</th>\n",
       "      <th>summary_wc_std</th>\n",
       "      <th>review_wc_std</th>\n",
       "      <th>year_std</th>\n",
       "      <th>group_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>sentence_wc</th>\n",
       "      <th>sentence_wc_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>2005</td>\n",
       "      <td>1114646400B000056OUGA3FFDK09UJS1TD</td>\n",
       "      <td>B000056OUG</td>\n",
       "      <td>A3FFDK09UJS1TD</td>\n",
       "      <td>[i, am, primarily, breastfeeding, ,, but, i, t...</td>\n",
       "      <td>[infrequent, bottle, user, ?, then, buy, steam...</td>\n",
       "      <td>10</td>\n",
       "      <td>84</td>\n",
       "      <td>1.560512</td>\n",
       "      <td>-0.494464</td>\n",
       "      <td>-0.845298</td>\n",
       "      <td>1</td>\n",
       "      <td>neg</td>\n",
       "      <td>[i, am, primarily, breastfeeding, but, i, thou...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.249451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>2005</td>\n",
       "      <td>1114646400B000056OUGA3FFDK09UJS1TD</td>\n",
       "      <td>B000056OUG</td>\n",
       "      <td>A3FFDK09UJS1TD</td>\n",
       "      <td>[i, am, primarily, breastfeeding, ,, but, i, t...</td>\n",
       "      <td>[infrequent, bottle, user, ?, then, buy, steam...</td>\n",
       "      <td>10</td>\n",
       "      <td>84</td>\n",
       "      <td>1.560512</td>\n",
       "      <td>-0.494464</td>\n",
       "      <td>-0.845298</td>\n",
       "      <td>1</td>\n",
       "      <td>neu</td>\n",
       "      <td>[i, discovered, medela, microwave, steam, clea...</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.118286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>2005</td>\n",
       "      <td>1114646400B000056OUGA3FFDK09UJS1TD</td>\n",
       "      <td>B000056OUG</td>\n",
       "      <td>A3FFDK09UJS1TD</td>\n",
       "      <td>[i, am, primarily, breastfeeding, ,, but, i, t...</td>\n",
       "      <td>[infrequent, bottle, user, ?, then, buy, steam...</td>\n",
       "      <td>10</td>\n",
       "      <td>84</td>\n",
       "      <td>1.560512</td>\n",
       "      <td>-0.494464</td>\n",
       "      <td>-0.845298</td>\n",
       "      <td>1</td>\n",
       "      <td>neu</td>\n",
       "      <td>[i, feel, badly, for, the, waste, of, money, b...</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.118286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>2005</td>\n",
       "      <td>1114646400B000056OUGA3FFDK09UJS1TD</td>\n",
       "      <td>B000056OUG</td>\n",
       "      <td>A3FFDK09UJS1TD</td>\n",
       "      <td>[i, am, primarily, breastfeeding, ,, but, i, t...</td>\n",
       "      <td>[infrequent, bottle, user, ?, then, buy, steam...</td>\n",
       "      <td>10</td>\n",
       "      <td>84</td>\n",
       "      <td>1.560512</td>\n",
       "      <td>-0.494464</td>\n",
       "      <td>-0.845298</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[and, not, space, consuming, .]</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.129561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>2005</td>\n",
       "      <td>1114646400B000056OUGA3FFDK09UJS1TD</td>\n",
       "      <td>B000056OUG</td>\n",
       "      <td>A3FFDK09UJS1TD</td>\n",
       "      <td>[i, am, primarily, breastfeeding, ,, but, i, t...</td>\n",
       "      <td>[infrequent, bottle, user, ?, then, buy, steam...</td>\n",
       "      <td>10</td>\n",
       "      <td>84</td>\n",
       "      <td>1.560512</td>\n",
       "      <td>-0.494464</td>\n",
       "      <td>-0.845298</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[plus, you, can, use, them, during, travel, at...</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.577956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  polarity  year                                uuid        asin  \\\n",
       "0           0  negative  2005  1114646400B000056OUGA3FFDK09UJS1TD  B000056OUG   \n",
       "1           0  negative  2005  1114646400B000056OUGA3FFDK09UJS1TD  B000056OUG   \n",
       "2           0  negative  2005  1114646400B000056OUGA3FFDK09UJS1TD  B000056OUG   \n",
       "3           0  negative  2005  1114646400B000056OUGA3FFDK09UJS1TD  B000056OUG   \n",
       "4           0  negative  2005  1114646400B000056OUGA3FFDK09UJS1TD  B000056OUG   \n",
       "\n",
       "       reviewerID                                      review_tokens  \\\n",
       "0  A3FFDK09UJS1TD  [i, am, primarily, breastfeeding, ,, but, i, t...   \n",
       "1  A3FFDK09UJS1TD  [i, am, primarily, breastfeeding, ,, but, i, t...   \n",
       "2  A3FFDK09UJS1TD  [i, am, primarily, breastfeeding, ,, but, i, t...   \n",
       "3  A3FFDK09UJS1TD  [i, am, primarily, breastfeeding, ,, but, i, t...   \n",
       "4  A3FFDK09UJS1TD  [i, am, primarily, breastfeeding, ,, but, i, t...   \n",
       "\n",
       "                                      summary_tokens  summary_wc  review_wc  \\\n",
       "0  [infrequent, bottle, user, ?, then, buy, steam...          10         84   \n",
       "1  [infrequent, bottle, user, ?, then, buy, steam...          10         84   \n",
       "2  [infrequent, bottle, user, ?, then, buy, steam...          10         84   \n",
       "3  [infrequent, bottle, user, ?, then, buy, steam...          10         84   \n",
       "4  [infrequent, bottle, user, ?, then, buy, steam...          10         84   \n",
       "\n",
       "   summary_wc_std  review_wc_std  year_std  group_id sentiment  \\\n",
       "0        1.560512      -0.494464 -0.845298         1       neg   \n",
       "1        1.560512      -0.494464 -0.845298         1       neu   \n",
       "2        1.560512      -0.494464 -0.845298         1       neu   \n",
       "3        1.560512      -0.494464 -0.845298         1       pos   \n",
       "4        1.560512      -0.494464 -0.845298         1       pos   \n",
       "\n",
       "                                     sentence_tokens  sentence_wc  \\\n",
       "0  [i, am, primarily, breastfeeding, but, i, thou...           20   \n",
       "1  [i, discovered, medela, microwave, steam, clea...           16   \n",
       "2  [i, feel, badly, for, the, waste, of, money, b...           16   \n",
       "3                    [and, not, space, consuming, .]            5   \n",
       "4  [plus, you, can, use, them, during, travel, at...           11   \n",
       "\n",
       "   sentence_wc_std  \n",
       "0         0.249451  \n",
       "1        -0.118286  \n",
       "2        -0.118286  \n",
       "3        -1.129561  \n",
       "4        -0.577956  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9429 tokens before discarding those that appear less than 2 times.\n",
      "5313 tokens after discarding those that appear less than 2 times.\n"
     ]
    }
   ],
   "source": [
    "### Preprocessing \n",
    "# declare the padding and unknown symbols\n",
    "pad_mask_int = 0\n",
    "pad_mask_sym = '==pad_mask=='\n",
    "unknown_int = 1\n",
    "unknown_sym = '==unknown_sym=='\n",
    "\n",
    "# vocabulary set\n",
    "vocab_counter = Counter()\n",
    "for doc in df['sentence_tokens']:\n",
    "    vocab_counter.update(doc)\n",
    "\n",
    "min_times_word_used = 2 # if at least 2 then the model will be prepared for unknown words in test and validation sets\n",
    "print(len(vocab_counter), \"tokens before discarding those that appear less than {} times.\".format(min_times_word_used))\n",
    "for key in list(vocab_counter.keys()):\n",
    "    if vocab_counter[key] < min_times_word_used: \n",
    "        vocab_counter.pop(key)\n",
    "print(len(vocab_counter), \"tokens after discarding those that appear less than {} times.\".format(min_times_word_used))   \n",
    "vocab_set = set(vocab_counter.keys())\n",
    "\n",
    "# vocabulary list and int map\n",
    "vocab_list = [pad_mask_sym, unknown_sym] + sorted(vocab_set)\n",
    "vocab_map = {word: index for index, word in enumerate(vocab_list)}\n",
    "\n",
    "# label set\n",
    "label_set = set(df['sentiment'].unique())\n",
    "\n",
    "# label list and int map\n",
    "label_list = sorted(label_set)\n",
    "label_map = {word: index for index, word in enumerate(label_list)}\n",
    "\n",
    "# polarity feature set\n",
    "polarity_set = set(df['polarity'].unique())\n",
    "\n",
    "# polarity list and int map\n",
    "polarity_list = sorted(polarity_set)\n",
    "polarity_map = {word: index for index, word in enumerate(polarity_list)}\n",
    "\n",
    "# group feature set\n",
    "group_set = set(df['group_id'].unique())\n",
    "\n",
    "# group list and int map\n",
    "group_list = sorted(group_set)\n",
    "group_map = {word: index for index, word in enumerate(group_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading big ol' word embeddings\n",
      "Loaded 1193514 word vectors.\n",
      "==pad_mask==\n",
      "==unknown_sym==\n",
      "(8\n",
      "):\n",
      ");\n",
      "->\n",
      "..\n",
      "0\n",
      "00\n",
      "06\n",
      "07\n",
      "1\n",
      "1-2\n",
      "1/2\n",
      "1/3\n",
      "1/4\n",
      "10\n",
      "10-12\n",
      "10-15\n",
      "100\n",
      "10oz\n",
      "11\n",
      "12\n",
      "12-18\n",
      "120\n",
      "13\n",
      "130\n",
      "14\n",
      "15\n",
      "150\n",
      "159\n",
      "16\n",
      "17\n",
      "18\n",
      "18mo\n",
      "19\n",
      "1st\n",
      "1yo\n",
      "2\n",
      "2-1\n",
      "2-3\n",
      "20\n",
      "200\n",
      "2002\n",
      "2004\n",
      "2005\n",
      "2007\n",
      "2009\n",
      "2012\n",
      "21\n",
      "22\n",
      "22lbs\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "29\n",
      "2nd\n",
      "2yo\n",
      "2yr\n",
      "3\n",
      "3-4\n",
      "3/4\n",
      "30\n",
      "30-45\n",
      "300\n",
      "32\n",
      "34\n",
      "35\n",
      "37\n",
      "39\n",
      "3m\n",
      "3rd\n",
      "3yo\n",
      "4\n",
      "4-5\n",
      "4-6\n",
      "40\n",
      "45\n",
      "4oz\n",
      "4th\n",
      "5\n",
      "5-10\n",
      "50\n",
      "500\n",
      "6\n",
      "6-7\n",
      "600\n",
      "6mo\n",
      "7\n",
      "7-8\n",
      "70\n",
      "75\n",
      "77\n",
      "7oz\n",
      "8\n",
      "8-9\n",
      "80\n",
      "8lbs\n",
      "8oz\n",
      "9\n",
      "90\n",
      "900\n",
      "900mhz\n",
      "99\n",
      "9oz\n",
      ":(\n",
      ":)\n",
      ":-)\n",
      "::\n",
      ":P\n",
      ";)\n",
      ";-)\n",
      "=)\n",
      "adiri\n",
      "ameda\n",
      "armholes\n",
      "attatch\n",
      "avents\n",
      "babiesrus\n",
      "baby-proofing\n",
      "babyhawk\n",
      "barely-used\n",
      "baseboards\n",
      "basinett\n",
      "born-free\n",
      "bouncenette\n",
      "breast-fed\n",
      "breastpump\n",
      "breastshield\n",
      "breastshields\n",
      "brown's\n",
      "car-seats\n",
      "cd's\n",
      "childproof\n",
      "colapsed\n",
      "colicky\n",
      "compactly\n",
      "cosleeper\n",
      "cosleeping\n",
      "cozyup\n",
      "cushie\n",
      "doens't\n",
      "dreamscapes\n",
      "drop-ins\n",
      "dropins\n",
      "engorged\n",
      "engorgement\n",
      "eurobath\n",
      "evenflow\n",
      "excema\n",
      "exersaucer\n",
      "flammability\n",
      "fraying\n",
      "freedislike\n",
      "freshners\n",
      "front-to-back\n",
      "furnature\n",
      "fussier\n",
      "fussiness\n",
      "heatable\n",
      "highchairs\n",
      "hygeia\n",
      "i'm\n",
      "i've\n",
      "indentations\n",
      "inverts\n",
      "iq24\n",
      "itzbeen\n",
      "jumperoo\n",
      "keyfit\n",
      "kiddopotamus\n",
      "kneeler\n",
      "kooshies\n",
      "lactina\n",
      "linkadoos\n",
      "medella\n",
      "microfleece\n",
      "middle-of-the-night\n",
      "milkmate\n",
      "months-old\n",
      "no-slip\n",
      "non-adjustable\n",
      "nurser\n",
      "nursers\n",
      "orignally\n",
      "pack-n-play\n",
      "platex\n",
      "playyard\n",
      "playyards\n",
      "positioner\n",
      "positioners\n",
      "pottys\n",
      "pre-shrunk\n",
      "prefolds\n",
      "prewashed\n",
      "pump-in-style\n",
      "pupsqueak\n",
      "rear-facing\n",
      "recevied\n",
      "recommed\n",
      "releive\n",
      "removeable\n",
      "safeseats\n",
      "santoprene\n",
      "seatback\n",
      "side-to-side\n",
      "sleepsacks\n",
      "slow-flow\n",
      "snackies\n",
      "snoogle\n",
      "snowsuits\n",
      "snuggli\n",
      "snugli\n",
      "snuzzler\n",
      "soothie\n",
      "soothies\n",
      "spigots\n",
      "spitup\n",
      "squirmer\n",
      "sterilizers\n",
      "sterilizes\n",
      "sterilizing\n",
      "sterlize\n",
      "sterlizer\n",
      "stinkier\n",
      "storkcraft\n",
      "sturdier\n",
      "suctioned\n",
      "suctioning\n",
      "suctions\n",
      "superyard\n",
      "swaddleme\n",
      "swaddler\n",
      "swaddles\n",
      "taggies\n",
      "teethers\n",
      "terrycloth\n",
      "tri-flow\n",
      "twist-tie\n",
      "twistaway\n",
      "unpadded\n",
      "unsnap\n",
      "unswaddled\n",
      "uppababy\n",
      "ventair\n",
      "ventaire\n",
      "ventaires\n",
      "volumne\n",
      "washables\n",
      "washings\n",
      "woombie\n",
      "wriggly\n",
      "you-shape\n",
      "255\n"
     ]
    }
   ],
   "source": [
    "# pretrained embeddings are from https://nlp.stanford.edu/projects/glove/\n",
    "# start by loading in the embedding matrix\n",
    "# load the whole embedding into memory\n",
    "print(\"\\nReading big ol' word embeddings\")\n",
    "count = 0\n",
    "embeddings_index_1 = dict()\n",
    "with open('../../../../data/external/glove.twitter.27B.50d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            print(values)\n",
    "        embeddings_index_1[word] = coefs\n",
    "print('Loaded %s word vectors.' % len(embeddings_index_1))\n",
    "\n",
    "#embeddings_index_2 = dict()\n",
    "#with open('../../../data/external/glove.twitter.27B.100d.txt') as f:\n",
    "#    for line in f:\n",
    "#        values = line.split()\n",
    "#        word = values[0]\n",
    "#        try:\n",
    "#            coefs = np.asarray(values[1:], dtype='float32')\n",
    "#        except:\n",
    "#            print(values)\n",
    "#        embeddings_index_2[word] = coefs\n",
    "#print('Loaded %s word vectors.' % len(embeddings_index_2))\n",
    "\n",
    "embedding_dim_1 = 50\n",
    "embedding_dim_2 = 0\n",
    "\n",
    "embedding_dim = embedding_dim_1 + embedding_dim_2\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "if embedding_dim_2 > 0:\n",
    "    embedding_matrix = np.zeros((len(vocab_list), embedding_dim))\n",
    "    for i, word in enumerate(vocab_list):\n",
    "        embedding_vector_1 = embeddings_index_1.get(word)\n",
    "        embedding_vector_2 = embeddings_index_2.get(word)\n",
    "        if embedding_vector_1 is not None and embedding_vector_2 is not None:\n",
    "            embedding_matrix[i] = np.concatenate((embedding_vector_1, embedding_vector_2))\n",
    "        elif embedding_vector_1 is None and embedding_vector_2 is not None:\n",
    "            embedding_matrix[i] = np.concatenate((np.zeros(embedding_dim_1), embedding_vector_2))        \n",
    "        elif embedding_vector_1 is not None and embedding_vector_2 is None:\n",
    "            embedding_matrix[i] = np.concatenate((embedding_vector_1, np.zeros(embedding_dim_2)))\n",
    "        else:\n",
    "            print(word)\n",
    "            count += 1 # maybe we should use fuzzywuzzy to get vector of nearest word? Instead of all zeros\n",
    "else:\n",
    "    embedding_matrix = np.zeros((len(vocab_list), embedding_dim))\n",
    "    for i, word in enumerate(vocab_list):\n",
    "        embedding_vector = embeddings_index_1.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            print(word)\n",
    "            count += 1 # maybe we should use fuzzywuzzy to get vector of nearest word? Instead of all zeros\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse \n",
    "from typing import List, Set, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "\n",
    "def create_one_hot(labels, label_dict: dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        labels:        array of labels, e.g. NumPy array or Pandas Series\n",
    "        label_dict:    dict of label indices\n",
    "    Return:\n",
    "        one_hot_numpy: sparse CSR 2d array of one-hot vectors\n",
    "    \"\"\"\n",
    "    one_hot_numpy = sparse.dok_matrix((len(labels), len(label_dict)), dtype=np.int8)\n",
    "    for i, label in enumerate(labels):\n",
    "        one_hot_numpy[i, label_dict[label]] = 1\n",
    "    return sparse.csr_matrix(one_hot_numpy) \n",
    "\n",
    "def undo_one_hot(pred, label_list: list) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    \n",
    "    Args: \n",
    "        pred:       NumPy array of one-hot predicted classes\n",
    "        label_list: a list of the label strings\n",
    "    Return:\n",
    "        label_pred: a list of predicted labels\n",
    "    \"\"\"\n",
    "    label_pred = [label_list[np.argmax(row)] for row in pred]\n",
    "    return label_pred\n",
    "    # this could probably be done awesomely fast as NumPy vectorised but it works\n",
    "\n",
    "\n",
    "def word_index(los: List[List[str]], vocab_dict: Dict[str, int], unknown: int, reverse: bool=False) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Replaces words with integers from a vocabulary dictionary or else with the integer for unknown\n",
    "    \n",
    "    Args:\n",
    "        los:     list of lists of split sentences\n",
    "        pad_to:  how big to make the padded list\n",
    "        unknown: the integer to put in for unknown tokens (either because they were pruned or not seen in training set)\n",
    "        reverse: reverse the order of tokens in the sub-list \n",
    "    Returns: \n",
    "        new_los: list of lists of split sentences where each token is replaced by an integer\n",
    "        \n",
    "    Examples:\n",
    "    >>> print(word_index([['one', 'two', 'three'], ['one', 'two']], {'one': 1, 'two': 2, 'three': 3}, unknown=4))\n",
    "    [[1, 2, 3], [1, 2]]\n",
    "    >>> print(word_index([['one', 'two', 'three'], ['one', 'two']], {'one': 1, 'two': 2, 'three': 3}, unknown=4, reverse=True))\n",
    "    [[3, 2, 1], [2, 1]]\n",
    "    \"\"\"\n",
    "    new_los = []\n",
    "    if reverse:\n",
    "        for sentence in los:\n",
    "            new_los.append([vocab_dict[word] if word in vocab_dict else unknown for word in sentence][::-1])        \n",
    "    else:\n",
    "        for sentence in los:\n",
    "            new_los.append([vocab_dict[word] if word in vocab_dict else unknown for word in sentence])\n",
    "    return new_los\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one-hot sparse matrix of labels\n",
    "y = create_one_hot(df['sentiment'], label_map).todense()\n",
    "\n",
    "# create one-hot of review polarity\n",
    "polarity = create_one_hot(df['polarity'], polarity_map)[:, 0].todense()\n",
    "\n",
    "# create one-hot of group number\n",
    "group = create_one_hot(df['group_id'], group_map).todense()\n",
    "\n",
    "\n",
    "# replace strings with ints (tokenization is done on the Series fed to word_index())\n",
    "sentences = word_index(df['sentence_tokens'], vocab_map, unknown_int, reverse=False)\n",
    "\n",
    "# pad / truncate \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentence_len = max(map(len, list(df['sentence_tokens'])))\n",
    "\n",
    "sentences = pad_sequences(sequences=sentences, \n",
    "                              maxlen=sentence_len, \n",
    "                              dtype='int32', \n",
    "                              padding='pre', \n",
    "                              value=pad_mask_int)\n",
    "\n",
    "#group = pad_sequences(sequences=group, \n",
    "#                              maxlen=embedding_dim, \n",
    "#                              dtype='int32', \n",
    "#                              padding='pre', \n",
    "#                              value=pad_mask_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0 2268\n",
      "   281 3490  665  750 2268 4718 4712 5260  493 4998 1852 4419 3546 3249\n",
      "   295 4682 3079  629   20]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0 2268 1356 2804 2830 4404  916  443 2368  295 4712 4704\n",
      "  5167 2302 4682  942   20]]\n",
      "[[1]\n",
      " [1]]\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:2])\n",
    "print(polarity[:2])\n",
    "print(y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_s (InputLayer)            (None, 159)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 159, 50)      265750      input_s[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 8)            1416        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_p (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_g (InputLayer)            (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 18)           0           gru_1[0][0]                      \n",
      "                                                                 input_p[0][0]                    \n",
      "                                                                 input_g[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            57          concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 267,223\n",
      "Trainable params: 1,473\n",
      "Non-trainable params: 265,750\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 13950 samples, validate on 1704 samples\n",
      "Epoch 1/50\n",
      "13950/13950 [==============================] - 41s 3ms/step - loss: 1.0467 - acc: 0.4627 - val_loss: 1.0402 - val_acc: 0.3668\n",
      "Epoch 2/50\n",
      "13950/13950 [==============================] - 39s 3ms/step - loss: 0.9467 - acc: 0.5753 - val_loss: 1.0049 - val_acc: 0.4531\n",
      "Epoch 3/50\n",
      "13950/13950 [==============================] - 38s 3ms/step - loss: 0.9075 - acc: 0.5961 - val_loss: 0.9737 - val_acc: 0.4930\n",
      "Epoch 4/50\n",
      "13950/13950 [==============================] - 37s 3ms/step - loss: 0.8790 - acc: 0.6081 - val_loss: 0.9366 - val_acc: 0.5205\n",
      "Epoch 5/50\n",
      "13950/13950 [==============================] - 36s 3ms/step - loss: 0.8563 - acc: 0.6160 - val_loss: 0.9062 - val_acc: 0.5511\n",
      "Epoch 6/50\n",
      "13950/13950 [==============================] - 37s 3ms/step - loss: 0.8355 - acc: 0.6293 - val_loss: 0.8865 - val_acc: 0.5505\n",
      "Epoch 7/50\n",
      "13950/13950 [==============================] - 37s 3ms/step - loss: 0.8239 - acc: 0.6356 - val_loss: 0.8599 - val_acc: 0.5704\n",
      "Epoch 8/50\n",
      "13950/13950 [==============================] - 37s 3ms/step - loss: 0.8148 - acc: 0.6429 - val_loss: 0.8519 - val_acc: 0.5839\n",
      "Epoch 9/50\n",
      "13950/13950 [==============================] - 40s 3ms/step - loss: 0.8019 - acc: 0.6493 - val_loss: 0.8363 - val_acc: 0.5986\n",
      "Epoch 10/50\n",
      "13950/13950 [==============================] - 40s 3ms/step - loss: 0.7991 - acc: 0.6528 - val_loss: 0.8323 - val_acc: 0.6062\n",
      "Epoch 11/50\n",
      "13950/13950 [==============================] - 41s 3ms/step - loss: 0.7889 - acc: 0.6576 - val_loss: 0.8242 - val_acc: 0.6080\n",
      "Epoch 12/50\n",
      "13950/13950 [==============================] - 40s 3ms/step - loss: 0.7851 - acc: 0.6560 - val_loss: 0.8221 - val_acc: 0.6074\n",
      "Epoch 13/50\n",
      "13950/13950 [==============================] - 40s 3ms/step - loss: 0.7796 - acc: 0.6681 - val_loss: 0.8167 - val_acc: 0.6180\n",
      "Epoch 14/50\n",
      "13950/13950 [==============================] - 42s 3ms/step - loss: 0.7739 - acc: 0.6676 - val_loss: 0.8055 - val_acc: 0.6197\n",
      "Epoch 15/50\n",
      "13950/13950 [==============================] - 41s 3ms/step - loss: 0.7722 - acc: 0.6676 - val_loss: 0.8008 - val_acc: 0.6279\n",
      "Epoch 16/50\n",
      "13950/13950 [==============================] - 40s 3ms/step - loss: 0.7681 - acc: 0.6727 - val_loss: 0.7992 - val_acc: 0.6268\n",
      "Epoch 17/50\n",
      "13950/13950 [==============================] - 42s 3ms/step - loss: 0.7652 - acc: 0.6672 - val_loss: 0.7966 - val_acc: 0.6215\n",
      "Epoch 18/50\n",
      "13950/13950 [==============================] - 40s 3ms/step - loss: 0.7601 - acc: 0.6778 - val_loss: 0.7894 - val_acc: 0.6362\n",
      "Epoch 19/50\n",
      "13888/13950 [============================>.] - ETA: 0s - loss: 0.7548 - acc: 0.6778"
     ]
    }
   ],
   "source": [
    "NAME = 'sentences-{}'.format(time.strftime('%y%m%d_%H%M', time.localtime(time.time())))\n",
    "copyfile(sys.argv[0], './tb_logs/{}.ipynb'.format(NAME))\n",
    "\n",
    "for g in range(1,10):\n",
    "    group_mask = df['group_id'] != g \n",
    "        \n",
    "    input_s = Input(shape=(sentence_len,), dtype='int32', name='input_s')\n",
    "    input_p = Input(shape=(1,), dtype='float32', name='input_p')\n",
    "    input_g = Input(shape=(len(group_list),), dtype='float32', name='input_g')\n",
    "\n",
    "    embedding_vector_length = embedding_dim\n",
    "    GRU_nodes_sentences = 8\n",
    "\n",
    "    emb = Embedding(len(vocab_list), embedding_vector_length, mask_zero=True,\n",
    "                        weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "    emb_s = emb(input_s)\n",
    "\n",
    "    gru_s = GRU(GRU_nodes_sentences,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            recurrent_initializer='orthogonal',\n",
    "            bias_initializer='zeros',\n",
    "            kernel_regularizer=None,\n",
    "            recurrent_regularizer=None,\n",
    "            bias_regularizer=L1L2(l1=0.1, l2=0.0),\n",
    "            activity_regularizer=L1L2(l1=1e-07, l2=0.0),\n",
    "            kernel_constraint=maxnorm(3),\n",
    "            recurrent_constraint=maxnorm(3),\n",
    "            bias_constraint=None,\n",
    "            return_sequences=False,\n",
    "            return_state=False,\n",
    "            go_backwards=False,\n",
    "            stateful=False,\n",
    "            dropout=0.3)(emb_s)\n",
    "\n",
    "    concat_1 = Concatenate()([gru_s, input_p, input_g])\n",
    "    output = Dense(len(label_set), activation='softmax')(concat_1)\n",
    "    model = Model([input_s, input_p, input_g], output)\n",
    "    nadam = keras.optimizers.nadam(lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "    tensorboard = TensorBoard(log_dir = './tb_logs/{}'.format('group_'+str(g)+'_'+NAME))\n",
    "\n",
    "    hist1 = model.fit(x=[sentences[group_mask], polarity[group_mask], group[group_mask]], \n",
    "                        y=y[group_mask], \n",
    "                        validation_data=([sentences[np.logical_not(group_mask)], \n",
    "                                          polarity[np.logical_not(group_mask)], \n",
    "                                          group[np.logical_not(group_mask)]], y[np.logical_not(group_mask)]), \n",
    "                        epochs=50, batch_size=64, callbacks=[es, tensorboard]) # , tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
