{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/raw/phase1_movie_reviews-train.csv')\n",
    "np.random.seed(42)\n",
    "df = df.sample(frac=1).reset_index(drop=True) # shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify by only working on small data set\n",
    "df = df.head(9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Don't take this movie serious...</td>\n",
       "      <td>Rating System:1 star = abysmal; some books des...</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>Ridley Scott's Graphic Masterpiece Comes To DVD</td>\n",
       "      <td>Ridley Scott's Graphic War Film Black Hawk Dow...</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>It'll be forgotten in ten years.</td>\n",
       "      <td>You know how every year, there are usually a f...</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>\"I miss you, Benny-boo-boo-boo!\"</td>\n",
       "      <td>\"How to Lose A Guy in Ten Days\" starts off awk...</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>Yawn</td>\n",
       "      <td>You could be forgiven after 15 minutes of this...</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                                          summary  \\\n",
       "0  negative                 Don't take this movie serious...   \n",
       "1  positive  Ridley Scott's Graphic Masterpiece Comes To DVD   \n",
       "2  negative                 It'll be forgotten in ten years.   \n",
       "3  negative                 \"I miss you, Benny-boo-boo-boo!\"   \n",
       "4  negative                                             Yawn   \n",
       "\n",
       "                                          reviewText  year  \n",
       "0  Rating System:1 star = abysmal; some books des...  2004  \n",
       "1  Ridley Scott's Graphic War Film Black Hawk Dow...  2002  \n",
       "2  You know how every year, there are usually a f...  2003  \n",
       "3  \"How to Lose A Guy in Ten Days\" starts off awk...  2005  \n",
       "4  You could be forgiven after 15 minutes of this...  2003  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9000 entries, 0 to 8999\n",
      "Data columns (total 4 columns):\n",
      "polarity      9000 non-null object\n",
      "summary       8999 non-null object\n",
      "reviewText    9000 non-null object\n",
      "year          9000 non-null int64\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 281.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This show lacks many things when compared to C...</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      polarity summary                                         reviewText  \\\n",
       "1796  negative     NaN  This show lacks many things when compared to C...   \n",
       "\n",
       "      year  \n",
       "1796  2004  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This show lacks many things when compared to CSI: namely acting, and good writing.  Curuso is a total bore, no personality and acts exactly the same as he did on NYPD Blue.  A total let down, but that was to be expected.  I would go so far as to say that this is a poor show, period.  I watched the entire first season, always waiting for it to get better.  It never did.  The only decent acting comes from Procter and the M.E. from News Radio, who is utterly unbelieveable in her role.  This is what happens when TV execs recycle TV shows.  Suprisingly, CSI NY is even worse, and I have tremendous respect for Sinise as an actor.Don't fake the funk.\n"
     ]
    }
   ],
   "source": [
    "print(df.at[1796, 'reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# social science coding random sampling code\n",
    "\n",
    "# np.random.seed(1980)\n",
    "# row_i = np.random.randint(0, high=len(df), size=20)\n",
    "# for i in row_i:\n",
    "#     print(\"Polarity: \", df.at[i, 'polarity'])\n",
    "#     print(\"Year: \", df.at[i, 'year'])\n",
    "#     print(\"Summary: \", df.at[i, 'summary'])\n",
    "#     print()\n",
    "#     print(\"Review: \", df.at[i, 'reviewText'])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_mask_int = 0\n",
    "pad_mask_sym = '==pad_mask=='\n",
    "# padleft_int = 0\n",
    "# padleft_sym = '==padleft_sym=='\n",
    "# padright_int = 1\n",
    "# padright_sym = '==padright_sym=='\n",
    "unknown_int = 1\n",
    "unknown_sym = '==unknown_sym=='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan with empty string\n",
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower-case?\n",
    "# for col in ['polarity', 'summary', 'reviewText']:\n",
    "#     df[col] = df[col].str.lower()\n",
    "# tokenizer does this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'movie', 'is', 'my', 'favorite', '!', '!', '!', 'i', '\"', 'love', '\"', 'it', 'so', 'much']\n"
     ]
    }
   ],
   "source": [
    "# tokenization \n",
    "\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\"\n",
    "    takes string input and tokenizes into a list of strings. \n",
    "    If runtime is slow move tknzr out of function and call for it in the input. \n",
    "    This is highly unlikely given that function runs in O(1) = constant time.  \n",
    "    \"\"\"\n",
    "    \n",
    "    string = re.sub('\\&quot;', '\"', string)\n",
    "    \n",
    "    tknzr = TweetTokenizer(preserve_case = False, strip_handles = True, reduce_len = True)\n",
    "    tokens = tknzr.tokenize(string)\n",
    "    return tokens\n",
    "\n",
    "print(tokenize('This movie is my favorite!!!!!!!! I &quot;love&quot; it so much'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.48 s, sys: 31.1 ms, total: 6.51 s\n",
      "Wall time: 6.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df[\"review_tokens\"] = df[\"reviewText\"].map(tokenize)\n",
    "df['summary_tokens'] =  df[\"summary\"].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ridley',\n",
       "  \"scott's\",\n",
       "  'graphic',\n",
       "  'war',\n",
       "  'film',\n",
       "  'black',\n",
       "  'hawk',\n",
       "  'down',\n",
       "  'is',\n",
       "  'a',\n",
       "  'masterpiece',\n",
       "  'on',\n",
       "  'every',\n",
       "  'level',\n",
       "  'of',\n",
       "  'film',\n",
       "  'making',\n",
       "  '.',\n",
       "  'it',\n",
       "  'intorduces',\n",
       "  'its',\n",
       "  'charcters',\n",
       "  'well',\n",
       "  'even',\n",
       "  'theo',\n",
       "  'we',\n",
       "  \"don't\",\n",
       "  'know',\n",
       "  'much',\n",
       "  'about',\n",
       "  'them',\n",
       "  '.',\n",
       "  'we',\n",
       "  'feel',\n",
       "  'what',\n",
       "  'they',\n",
       "  'half',\n",
       "  'to',\n",
       "  'go',\n",
       "  'through',\n",
       "  'in',\n",
       "  'the',\n",
       "  'film',\n",
       "  '.',\n",
       "  'what',\n",
       "  'i',\n",
       "  'liked',\n",
       "  'most',\n",
       "  'about',\n",
       "  'the',\n",
       "  'film',\n",
       "  'is',\n",
       "  'that',\n",
       "  'it',\n",
       "  'shows',\n",
       "  'the',\n",
       "  'level',\n",
       "  'of',\n",
       "  'cauos',\n",
       "  'that',\n",
       "  'war',\n",
       "  'is',\n",
       "  '.',\n",
       "  'when',\n",
       "  'something',\n",
       "  'goes',\n",
       "  'wrong',\n",
       "  'these',\n",
       "  'charcters',\n",
       "  'act',\n",
       "  'like',\n",
       "  'real',\n",
       "  'people',\n",
       "  'would',\n",
       "  'they',\n",
       "  'act',\n",
       "  'scared',\n",
       "  ',',\n",
       "  'afride',\n",
       "  ',',\n",
       "  'and',\n",
       "  'nervous',\n",
       "  '.',\n",
       "  'you',\n",
       "  'mite',\n",
       "  'not',\n",
       "  'know',\n",
       "  'these',\n",
       "  'charcters',\n",
       "  'well',\n",
       "  'enough',\n",
       "  '.',\n",
       "  'but',\n",
       "  'you',\n",
       "  'no',\n",
       "  'them',\n",
       "  'enough',\n",
       "  'to',\n",
       "  'feel',\n",
       "  'for',\n",
       "  'them',\n",
       "  'when',\n",
       "  'they',\n",
       "  'get',\n",
       "  'hurt',\n",
       "  'or',\n",
       "  'killed',\n",
       "  '.',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'based',\n",
       "  'on',\n",
       "  'true',\n",
       "  'events',\n",
       "  '.',\n",
       "  'a',\n",
       "  'word',\n",
       "  'of',\n",
       "  'warning',\n",
       "  'the',\n",
       "  'violence',\n",
       "  'in',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'extremely',\n",
       "  'graphic',\n",
       "  'and',\n",
       "  'prolonged',\n",
       "  'and',\n",
       "  'is',\n",
       "  'not',\n",
       "  'for',\n",
       "  'the',\n",
       "  'faint',\n",
       "  'at',\n",
       "  'heart',\n",
       "  '.',\n",
       "  'in',\n",
       "  'one',\n",
       "  'prolonged',\n",
       "  'scene',\n",
       "  'a',\n",
       "  'soilder',\n",
       "  'is',\n",
       "  'wouded',\n",
       "  'and',\n",
       "  'the',\n",
       "  'other',\n",
       "  'soilders',\n",
       "  'must',\n",
       "  'operate',\n",
       "  'on',\n",
       "  'him',\n",
       "  'or',\n",
       "  'he',\n",
       "  'will',\n",
       "  'die',\n",
       "  '.',\n",
       "  'the',\n",
       "  'camara',\n",
       "  'lingers',\n",
       "  'on',\n",
       "  'the',\n",
       "  'wound',\n",
       "  'as',\n",
       "  'blood',\n",
       "  'gushes',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'wound',\n",
       "  'we',\n",
       "  'as',\n",
       "  'a',\n",
       "  'movie',\n",
       "  'audience',\n",
       "  'must',\n",
       "  'witness',\n",
       "  'this',\n",
       "  'for',\n",
       "  'awhile',\n",
       "  'it',\n",
       "  'last',\n",
       "  'saverel',\n",
       "  'minutes',\n",
       "  'and',\n",
       "  'that',\n",
       "  'is',\n",
       "  'just',\n",
       "  'one',\n",
       "  'scene',\n",
       "  '.',\n",
       "  'this',\n",
       "  'is',\n",
       "  'by',\n",
       "  'far',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'best',\n",
       "  'war',\n",
       "  \"film's\",\n",
       "  'i',\n",
       "  'have',\n",
       "  'seen',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'masterpiece',\n",
       "  '.',\n",
       "  'but',\n",
       "  'again',\n",
       "  'this',\n",
       "  'is',\n",
       "  'not',\n",
       "  'a',\n",
       "  'movie',\n",
       "  'for',\n",
       "  'people',\n",
       "  'with',\n",
       "  'weak',\n",
       "  \"stomic's\",\n",
       "  '.']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization example\n",
    "list(df.loc[1:1, 'review_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word count columns \n",
    "df['summary_wc'] = df['summary_tokens'].map(len)\n",
    "df['review_wc'] = df['review_tokens'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise continuous columns \n",
    "mean_summary_wc = np.mean(df['summary_wc'])\n",
    "mean_review_wc = np.mean(df['review_wc'])\n",
    "mean_year = np.mean(df['year'])\n",
    "\n",
    "std_summary_wc = np.std(df['summary_wc'])\n",
    "std_review_wc = np.std(df['review_wc'])\n",
    "std_year = np.std(df['year'])\n",
    "\n",
    "df['summary_wc'] = (df['summary_wc'] - mean_summary_wc) / std_summary_wc\n",
    "df['review_wc'] = (df['review_wc'] - mean_review_wc) / std_review_wc\n",
    "df['year'] = (df['year'] - mean_year) / std_year\n",
    "\n",
    "# if you don't the training is completely fucked and forever stuck at 0.5 accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into train/test/validation...\n"
     ]
    }
   ],
   "source": [
    "# train, validation, test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Splitting into train/test/validation...\")\n",
    "train, test = train_test_split(df, test_size=0.2,random_state = 7)\n",
    "validation, test = train_test_split(test, test_size=0.5, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8770   -0.137212\n",
       "3929   -0.840514\n",
       "1096   -0.608838\n",
       "4431   -0.331654\n",
       "6784   -0.207542\n",
       "3529    0.065505\n",
       "8946    0.487486\n",
       "5464   -0.414395\n",
       "6401   -0.315106\n",
       "2684    2.245740\n",
       "5589   -0.439218\n",
       "6947   -0.447492\n",
       "2223   -0.364751\n",
       "4898   -0.195131\n",
       "4384   -0.869473\n",
       "7798   -0.186857\n",
       "3063   -0.675031\n",
       "5174    0.218576\n",
       "5841    0.408881\n",
       "3722    1.459697\n",
       "2551   -0.753635\n",
       "6215   -0.368888\n",
       "4310    0.156520\n",
       "1257   -0.385436\n",
       "934     2.729777\n",
       "2169   -0.228227\n",
       "8981   -0.848788\n",
       "5072   -0.356477\n",
       "2405    0.152383\n",
       "842    -0.555056\n",
       "          ...   \n",
       "6736   -0.861199\n",
       "3389    0.214439\n",
       "8052   -0.389573\n",
       "6304   -0.844651\n",
       "5270    0.214439\n",
       "187    -0.546782\n",
       "5693   -0.749498\n",
       "1587   -0.373025\n",
       "788     1.219747\n",
       "8250    0.185480\n",
       "7632   -0.844651\n",
       "1127   -0.799143\n",
       "1901   -0.745361\n",
       "6846   -0.724676\n",
       "582    -0.865336\n",
       "3709   -0.699853\n",
       "4797   -0.782595\n",
       "6623   -0.848788\n",
       "2495   -0.737087\n",
       "247     0.086190\n",
       "732    -0.282009\n",
       "2235   -0.037922\n",
       "6913    3.966761\n",
       "3213    2.489827\n",
       "1538   -0.981174\n",
       "1787   -0.509548\n",
       "5577    0.479211\n",
       "5448   -0.869473\n",
       "1435    0.470937\n",
       "5379    0.599186\n",
       "Name: review_wc, Length: 900, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['review_wc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66147 tokens before discarding those that appear less than 2 times.\n",
      "30097 tokens after discarding those that appear less than 2 times.\n"
     ]
    }
   ],
   "source": [
    "# vocabulary set\n",
    "vocab_counter = Counter()\n",
    "for doc in train['summary_tokens']:\n",
    "    vocab_counter.update(doc)\n",
    "for doc in train['review_tokens']:\n",
    "    vocab_counter.update(doc)    \n",
    "\n",
    "min_times_word_used = 2 # if at least 2 then the model will be prepared for unknown words in test and validation sets\n",
    "print(len(vocab_counter), \"tokens before discarding those that appear less than {} times.\".format(min_times_word_used))\n",
    "for key in list(vocab_counter.keys()):\n",
    "    if vocab_counter[key] < min_times_word_used: \n",
    "        vocab_counter.pop(key)\n",
    "print(len(vocab_counter), \"tokens after discarding those that appear less than {} times.\".format(min_times_word_used))   \n",
    "vocab_set = set(vocab_counter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary list and int map\n",
    "vocab_list = [pad_mask_sym, unknown_sym] + sorted(vocab_set)\n",
    "vocab_map = {word: index for index, word in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==unknown_sym==\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vocab_list[1])\n",
    "vocab_map['==unknown_sym==']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label set\n",
    "label_set = set(train['polarity'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label list and int map\n",
    "label_list = sorted(label_set)\n",
    "label_map = {word: index for index, word in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polarity to 0 / 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse \n",
    "\n",
    "def create_one_hot(labels, label_dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        labels:        list of labels\n",
    "        label_dict:    dict of label indices\n",
    "    Return:\n",
    "        one_hot_numpy: sparse CSR 2d array of one-hot vectors\n",
    "    \"\"\"\n",
    "    one_hot_numpy = sparse.dok_matrix((len(labels), len(label_dict)), dtype=np.int8)\n",
    "    for i, label in enumerate(labels):\n",
    "        one_hot_numpy[i, label_dict[label]] = 1\n",
    "    return sparse.csr_matrix(one_hot_numpy) \n",
    "\n",
    "def undo_one_hot(pred, label_list):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args: \n",
    "        pred: NumPy array of one-hot predicted classes\n",
    "        label_list: a list of the label strings\n",
    "    Return:\n",
    "        label_pred: an NumPy array of predicted labels\n",
    "    \"\"\"\n",
    "    labels = np.array(label_list)\n",
    "    label_pred = labels[np.argmax(pred, axis=1)]\n",
    "    return label_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = create_one_hot(train['polarity'], label_map)\n",
    "y_validation = create_one_hot(validation['polarity'], label_map)\n",
    "y_test = create_one_hot(test['polarity'], label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        ...,\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0]], dtype=int8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace strings with ints \n",
    "from typing import List, Set, Dict, Tuple, Optional\n",
    "\n",
    "def word_index(los: List[List[str]], vocab_dict: Dict[str, int], unknown: int, reverse: bool=False) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Replaces words with integers from a vocabulary dictionary or else with 1+ number of keys in dictionary\n",
    "    \n",
    "    Args:\n",
    "        los: list of lists of split sentences\n",
    "        pad_to: how big to make the padded list\n",
    "    Returns: \n",
    "        new_los: list of lists of split sentences wrapped around\n",
    "        \n",
    "    Examples:\n",
    "    print(word_index([['one', 'two', 'three'], ['one', 'two']], {'one': 1, 'two': 2, 'three': 3}))\n",
    "    \"\"\"\n",
    "    new_los = []\n",
    "    if reverse:\n",
    "        for sentence in los:\n",
    "            new_los.append(reversed([vocab_dict[word] if word in vocab_dict else unknown for word in sentence]))        \n",
    "    else:\n",
    "        for sentence in los:\n",
    "            new_los.append([vocab_dict[word] if word in vocab_dict else unknown for word in sentence])\n",
    "    return new_los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [1, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(word_index([['one', 'two', 'three'], ['one', 'two']], {'one': 1, 'two': 2, 'three': 3}, unknown=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summary = word_index(train['summary_tokens'], vocab_map, unknown_int)\n",
    "train_review = word_index(train['review_tokens'], vocab_map, unknown_int) \n",
    "\n",
    "validation_summary = word_index(validation['summary_tokens'], vocab_map, unknown_int)\n",
    "validation_review = word_index(validation['review_tokens'], vocab_map, unknown_int) \n",
    "\n",
    "test_summary = word_index(test['summary_tokens'], vocab_map, unknown_int)\n",
    "test_review = word_index(test['review_tokens'], vocab_map, unknown_int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# pad / truncate \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "summary_len = max(map(len, list(train['summary_tokens'])))\n",
    "review_len = 500\n",
    "\n",
    "train_summary = pad_sequences(sequences=train_summary, \n",
    "                              maxlen=summary_len, \n",
    "                              dtype='int32', \n",
    "                              padding='pre', \n",
    "                              value=pad_mask_int)\n",
    "train_review = pad_sequences(sequences=train_review, \n",
    "                             maxlen=review_len, \n",
    "                             dtype='int32', \n",
    "                             padding='pre',\n",
    "                             truncating='pre',\n",
    "                             value=pad_mask_int)\n",
    "\n",
    "validation_summary = pad_sequences(sequences=validation_summary, \n",
    "                              maxlen=summary_len, \n",
    "                              dtype='int32', \n",
    "                              padding='pre', \n",
    "                              value=pad_mask_int)\n",
    "validation_review = pad_sequences(sequences=validation_review, \n",
    "                             maxlen=review_len, \n",
    "                             dtype='int32', \n",
    "                             padding='pre',\n",
    "                             truncating='pre',\n",
    "                             value=pad_mask_int)\n",
    "\n",
    "test_summary = pad_sequences(sequences=test_summary, \n",
    "                              maxlen=summary_len, \n",
    "                              dtype='int32', \n",
    "                              padding='pre', \n",
    "                              value=pad_mask_int)\n",
    "test_review = pad_sequences(sequences=test_review, \n",
    "                             maxlen=review_len, \n",
    "                             dtype='int32', \n",
    "                             padding='pre',\n",
    "                             truncating='pre',\n",
    "                             value=pad_mask_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "5694\n",
      "30099\n"
     ]
    }
   ],
   "source": [
    "print(max(map(len, list(df['summary_tokens']))))\n",
    "print(max(map(len, list(df['review_tokens']))))\n",
    "print(len(vocab_list))\n",
    "# could simplify this now we have a wc column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot if not using embeddings (a Keras embedding layer can actually also do one-hot for you so...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.optimizers\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Concatenate \n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.constraints import maxnorm\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "# fix random seed for reproducibility - only works for CPU version of tensorflow\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "import itertools\n",
    "\n",
    "def plot_results(losses, accuracies):\n",
    "    fig,ax = plt.subplots(1,2,figsize=[12,2])\n",
    "    ax[0].plot(losses)\n",
    "    ax[0].set_ylabel('loss')\n",
    "    ax[0].set_xlabel('iteration');\n",
    "    ax[1].plot(accuracies);\n",
    "    ax[1].set_ylabel('accuracy')\n",
    "    ax[1].set_xlabel('iteration');\n",
    "    \n",
    "    \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    From: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#        print(\"Normalized confusion matrix\")\n",
    "#    else:\n",
    "#        print('Confusion matrix, without normalization')\n",
    "#\n",
    "#    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if cm[i, j] != 0:\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "def plot_confusion(y, y_pred, label_list) -> None:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y: true labels\n",
    "        y_pred: predicted labels\n",
    "        label_list: ordered iterable of labels\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = sklearn.metrics.confusion_matrix(y, y_pred, labels=label_list)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plt.figure(figsize=(13,10))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=label_list,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plt.figure(figsize=(13,10))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=label_list, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1917494 word vectors.\n",
      "CPU times: user 1min 38s, sys: 1.91 s, total: 1min 40s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# pretrained embeddings are from https://nlp.stanford.edu/projects/glove/\n",
    "# start by loading in the embedding matrix\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "with open('../../data/external/glove.42B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==pad_mask==\n",
      "==unknown_sym==\n",
      "#10\n",
      "#14\n",
      "#16\n",
      "#17\n",
      "#24\n",
      "#u-control\n",
      "(8\n",
      ")8\n",
      "):\n",
      ");\n",
      "*\n",
      ".  ...\n",
      ". .\n",
      ". . .\n",
      ". ...\n",
      "..\n",
      "... ...\n",
      "/8\n",
      "1-2-\n",
      "100.00+\n",
      "2.the\n",
      "6victoria\n",
      "6vs\n",
      "7vs\n",
      "8)\n",
      "8/\n",
      "8:\n",
      "8]\n",
      "8vs\n",
      "9vs\n",
      ":(\n",
      ":-(\n",
      "::\n",
      ":D\n",
      ":P\n",
      "<spoiler>\n",
      "_cabaret_\n",
      "_lolita_\n",
      "a's\n",
      "a-res\n",
      "abc's\n",
      "aboriginee\n",
      "academybest\n",
      "acornmedia\n",
      "acting.overall\n",
      "acting.the\n",
      "action-fest\n",
      "actor's\n",
      "actor.it\n",
      "adam's\n",
      "add.the\n",
      "administration's\n",
      "adventures.reed\n",
      "affleck's\n",
      "africa's\n",
      "agent's\n",
      "ain't\n",
      "akhenaton's\n",
      "aki's\n",
      "akio's\n",
      "al's\n",
      "alcott's\n",
      "alec's\n",
      "alex's\n",
      "alexander's\n",
      "algren's\n",
      "ali's\n",
      "alice's\n",
      "alien's\n",
      "alien.the\n",
      "aliens-are-coming-to-kill-us\n",
      "all's\n",
      "allen's\n",
      "alma's\n",
      "almereyda's\n",
      "alpha's\n",
      "altman's\n",
      "amazon's\n",
      "ambril\n",
      "amelia's\n",
      "america's\n",
      "american's\n",
      "amidala's\n",
      "amigo's\n",
      "anakin's\n",
      "anderson's\n",
      "andoheb\n",
      "andy's\n",
      "angel's\n",
      "angle's\n",
      "ani's\n",
      "animation's\n",
      "anime's\n",
      "aniston's\n",
      "ann's\n",
      "ann-margret's\n",
      "anna's\n",
      "anne's\n",
      "anne-moss\n",
      "annie's\n",
      "anniston's\n",
      "another's\n",
      "anotonius\n",
      "anti-normal\n",
      "anton's\n",
      "antonio's\n",
      "antony's\n",
      "anybody's\n",
      "anyone's\n",
      "aoyama's\n",
      "architect's\n",
      "aren't\n",
      "argento's\n",
      "ariannus\n",
      "army's\n",
      "arn't\n",
      "arnetia\n",
      "arnie's\n",
      "arnold's\n",
      "arquette's\n",
      "arthur's\n",
      "artist's\n",
      "ash's\n",
      "asimov's\n",
      "asleep.and\n",
      "asleep.it\n",
      "astaire's\n",
      "attkinson\n",
      "atwood's\n",
      "audience's\n",
      "audioworld\n",
      "audley's\n",
      "audrey's\n",
      "austen's\n",
      "australia's\n",
      "author's\n",
      "avery's\n",
      "avner's\n",
      "avril's\n",
      "b-flicks\n",
      "baby's\n",
      "back.while\n",
      "backlinie\n",
      "bacon's\n",
      "badly-produced\n",
      "bagpie\n",
      "bailey's\n",
      "baker's\n",
      "baketamon\n",
      "bakshi's\n",
      "baldwin's\n",
      "bale's\n",
      "balok's\n",
      "bana's\n",
      "band's\n",
      "banky's\n",
      "banner's\n",
      "barbra's\n",
      "barker's\n",
      "barnaby's\n",
      "barry's\n",
      "barrymore's\n",
      "bart's\n",
      "bartoli's\n",
      "barzell\n",
      "basinger's\n",
      "bava's\n",
      "baxter's\n",
      "bay's\n",
      "bbc's\n",
      "becky's\n",
      "beek's\n",
      "beethoven's\n",
      "bela's\n",
      "belgistan\n",
      "believable.the\n",
      "belle's\n",
      "ben's\n",
      "bening's\n",
      "bennett's\n",
      "benny's\n",
      "bergman's\n",
      "berkley's\n",
      "bernie's\n",
      "bernstein's\n",
      "berry's\n",
      "besson's\n",
      "beth's\n",
      "bette's\n",
      "betty's\n",
      "beyond-return\n",
      "bible's\n",
      "bigelow's\n",
      "bill's\n",
      "billy's\n",
      "bishop's\n",
      "bison's\n",
      "bl33p\n",
      "black's\n",
      "blackwolf's\n",
      "blade's\n",
      "blanchart\n",
      "blatty's\n",
      "bleccchhh\n",
      "blithers\n",
      "bloom's\n",
      "bloomingdale's\n",
      "bluebeard's\n",
      "boat's\n",
      "bob's\n",
      "bobby's\n",
      "bobtec\n",
      "bolt's\n",
      "bomed\n",
      "bond's\n",
      "book's\n",
      "boorman's\n",
      "borans\n",
      "boring.there\n",
      "borscht-belt\n",
      "boss's\n",
      "bourne's\n",
      "bow's\n",
      "boy's\n",
      "boyfriend's\n",
      "boyle's\n",
      "bradbury's\n",
      "brady's\n",
      "braff's\n",
      "branagh's\n",
      "brando's\n",
      "brannagh's\n",
      "brawlers.pride\n",
      "breaveheart\n",
      "bretaigne\n",
      "bridge's\n",
      "briggite\n",
      "broderick's\n",
      "brody's\n",
      "broker's\n",
      "bronson's\n",
      "brooks's\n",
      "broomfield's\n",
      "brosnan's\n",
      "brother's\n",
      "brown's\n",
      "browning's\n",
      "bruce's\n",
      "bruckheimer's\n",
      "buchanan's\n",
      "buck's\n",
      "bueller's\n",
      "buffy's\n",
      "bug's\n",
      "build's\n",
      "bullock's\n",
      "bundage\n",
      "burke's\n",
      "burns's\n",
      "burt's\n",
      "burton's\n",
      "bush's\n",
      "buzz's\n",
      "caesar's\n",
      "caeser's\n",
      "cage's\n",
      "cagney's\n",
      "caine's\n",
      "calhern's\n",
      "calvin's\n",
      "camera's\n",
      "cameron's\n",
      "campyness\n",
      "can't-miss\n",
      "candy's\n",
      "cannibal's\n",
      "capra's\n",
      "captain's\n",
      "cara's\n",
      "carcaterra's\n",
      "career.there\n",
      "carey's\n",
      "carl's\n",
      "carla's\n",
      "carlito's\n",
      "carlyle's\n",
      "carnahan's\n",
      "carney's\n",
      "carpenter's\n",
      "carrie's\n",
      "carroll's\n",
      "carvey's\n",
      "cassevettes\n",
      "cassidy's\n",
      "cast's\n",
      "castle's\n",
      "cate's\n",
      "catherine's\n",
      "cavandish\n",
      "cd's\n",
      "celie's\n",
      "celluoid\n",
      "championshipchris\n",
      "championshipkurt\n",
      "chan's\n",
      "chancerloville\n",
      "chandler's\n",
      "chaney's\n",
      "channe's\n",
      "channel's\n",
      "chaplin's\n",
      "character's\n",
      "characters.while\n",
      "charictors\n",
      "charley's\n",
      "charlie's\n",
      "charlotte's\n",
      "charture\n",
      "chase's\n",
      "chest-burster\n",
      "chicago's\n",
      "chieh's\n",
      "child's\n",
      "children's\n",
      "chokeslamming\n",
      "chris's\n",
      "christ's\n",
      "christensen's\n",
      "christina's\n",
      "christopher's\n",
      "chrysagon's\n",
      "chuck's\n",
      "chucky's\n",
      "church's\n",
      "cimino's\n",
      "cinamatography\n",
      "cinderella's\n",
      "cindy's\n",
      "cinema.as\n",
      "cinemaphiles\n",
      "city's\n",
      "ciupka\n",
      "clancy's\n",
      "clarice's\n",
      "clarissa's\n",
      "clark's\n",
      "clash's\n",
      "classic--and\n",
      "classic.it\n",
      "clench-jawed\n",
      "cleo's\n",
      "cliche's\n",
      "cliff's\n",
      "clint's\n",
      "clooney's\n",
      "club's\n",
      "coach's\n",
      "coburn's\n",
      "cocteau's\n",
      "coen's\n",
      "coldrating\n",
      "cole's\n",
      "collector's\n",
      "columbus's\n",
      "comady's\n",
      "comedy.but\n",
      "commendatori\n",
      "company's\n",
      "competitor's\n",
      "computer's\n",
      "comrodery\n",
      "connery's\n",
      "connor's\n",
      "conrad's\n",
      "convincing.the\n",
      "cop's\n",
      "coppola's\n",
      "cora's\n",
      "cordelia's\n",
      "corey's\n",
      "corman's\n",
      "corpse's\n",
      "costello's\n",
      "costner's\n",
      "costumer's\n",
      "cotillard's\n",
      "cotten's\n",
      "coulardeau\n",
      "could'nt\n",
      "could've\n",
      "couldn't\n",
      "count's\n",
      "country's\n",
      "couple's\n",
      "cox's\n",
      "craig's\n",
      "crane's\n",
      "craven's\n",
      "crawford's\n",
      "creature's\n",
      "crew's\n",
      "crichton's\n",
      "criminal's\n",
      "criminal.the\n",
      "criterion's\n",
      "critic's\n",
      "cronenberg's\n",
      "crow's\n",
      "crowe's\n",
      "cruise's\n",
      "crusader's\n",
      "crusher's\n",
      "cruz's\n",
      "crystal's\n",
      "cuaro'n\n",
      "cube's\n",
      "cuckoo's\n",
      "culture's\n",
      "curly's\n",
      "curtis's\n",
      "curwyn\n",
      "cusack's\n",
      "cybertron's\n",
      "dad's\n",
      "daddy's\n",
      "dafoe's\n",
      "damme's\n",
      "damon's\n",
      "dan's\n",
      "danielle's\n",
      "danny's\n",
      "dante's\n",
      "darcy's\n",
      "darin's\n",
      "dario's\n",
      "darwin's\n",
      "dat's\n",
      "data's\n",
      "daughter's\n",
      "dave's\n",
      "david's\n",
      "david.as\n",
      "davy's\n",
      "dawson's\n",
      "day's\n",
      "de-niro\n",
      "dead's\n",
      "dean's\n",
      "deborth\n",
      "decade's\n",
      "decoteau's\n",
      "dee's\n",
      "defeo's\n",
      "deht\n",
      "demme's\n",
      "deniro's\n",
      "dentist's\n",
      "denzel's\n",
      "depalma's\n",
      "depp's\n",
      "devil's\n",
      "dicaprio's\n",
      "dick's\n",
      "dicken's\n",
      "dickey's\n",
      "dickie's\n",
      "diesal's\n",
      "diesel's\n",
      "dieter's\n",
      "diolauge\n",
      "diolouge\n",
      "dion's\n",
      "diplomat's\n",
      "directing.the\n",
      "director's\n",
      "disappoint.the\n",
      "disney's\n",
      "disqo\n",
      "dissapointed.the\n",
      "distantiation\n",
      "dj's\n",
      "doc's\n",
      "doctor's\n",
      "does'nt\n",
      "does't\n",
      "doesen't\n",
      "dog's\n",
      "doll's\n",
      "dolores's\n",
      "don's\n",
      "donne's\n",
      "donnie's\n",
      "donor's\n",
      "donovan's\n",
      "doom's\n",
      "dorn's\n",
      "dosen't\n",
      "dosn't\n",
      "dot-com's\n",
      "douglas's\n",
      "downey's\n",
      "doyle's\n",
      "dq'ed\n",
      "dr.lorca\n",
      "dracula's\n",
      "driver's\n",
      "droxine\n",
      "dryfuss\n",
      "dubelko\n",
      "dudley's\n",
      "duff's\n",
      "duke's\n",
      "dumb.the\n",
      "dunaway's\n",
      "duvall's\n",
      "dvd's\n",
      "dvd.if\n",
      "dvd.this\n",
      "dyke's\n",
      "dylan's\n",
      "e's\n",
      "earth's\n",
      "eastwood's\n",
      "ebert's\n",
      "eckhart's\n",
      "eckland's\n",
      "ed's\n",
      "eddie's\n",
      "edition's\n",
      "editor's\n",
      "edna's\n",
      "eggar's\n",
      "egoyan's\n",
      "egypt's\n",
      "elfman's\n",
      "elisabeth's\n",
      "eliza's\n",
      "elizabeth's\n",
      "elle's\n",
      "elmo's\n",
      "else's\n",
      "elvis's\n",
      "embarrassment.the\n",
      "eminem's\n",
      "emperor's\n",
      "empire's\n",
      "employee's\n",
      "end.all\n",
      "end.great\n",
      "ending's\n",
      "ending.this\n",
      "endo-skeleton\n",
      "england's\n",
      "enterprise's\n",
      "entertainment's\n",
      "episode's\n",
      "era's\n",
      "espandi\n",
      "euro-sleaze\n",
      "evening's\n",
      "everybody's\n",
      "everyone's\n",
      "everything's\n",
      "evil.although\n",
      "ewan's\n",
      "extra's\n",
      "extras--including\n",
      "f--king\n",
      "failibrium\n",
      "fair-handed\n",
      "family's\n",
      "fan's\n",
      "faramir's\n",
      "farley's\n",
      "farmer's\n",
      "farrell's\n",
      "farrely\n",
      "father's\n",
      "faultpriests\n",
      "faye's\n",
      "fear.com\n",
      "fellini's\n",
      "feminum\n",
      "figure's\n",
      "film's\n",
      "film--and\n",
      "film--it's\n",
      "film.all\n",
      "film.although\n",
      "film.anyway\n",
      "film.first\n",
      "film.my\n",
      "film.not\n",
      "film.overall\n",
      "film.sam\n",
      "film.so\n",
      "film.then\n",
      "film.there\n",
      "film.what\n",
      "film.while\n",
      "filmand\n",
      "filmmaker's\n",
      "films.but\n",
      "films.it\n",
      "films.so\n",
      "films.this\n",
      "finian's\n",
      "finney's\n",
      "fiona's\n",
      "firm's\n",
      "fish's\n",
      "fleischer's\n",
      "fleming's\n",
      "fleur's\n",
      "flint's\n",
      "flynn's\n",
      "foley's\n",
      "follow.if\n",
      "fonda's\n",
      "force.as\n",
      "ford's\n",
      "forman's\n",
      "former's\n",
      "formulistic\n",
      "forster's\n",
      "fort's\n",
      "foster's\n",
      "four's\n",
      "fowler's\n",
      "fox's\n",
      "foyle's\n",
      "francesca's\n",
      "francie's\n",
      "frank's\n",
      "frankenstein's\n",
      "frankie's\n",
      "frasier's\n",
      "freddy's\n",
      "freeman's\n",
      "fresh.what\n",
      "friend's\n",
      "frightening.it\n",
      "frodo's\n",
      "frog-voiced\n",
      "fulci's\n",
      "fulcis\n",
      "fuller's\n",
      "funny.if\n",
      "funny.there\n",
      "gable's\n",
      "galaxies-spanning\n",
      "galsworthy's\n",
      "gangster's\n",
      "garland's\n",
      "garnett's\n",
      "garrett's\n",
      "garrison's\n",
      "garth's\n",
      "gbdisc\n",
      "gblouse\n",
      "gbtotal\n",
      "gedren\n",
      "gehrig's\n",
      "gem.it\n",
      "gene's\n",
      "general's\n",
      "generation's\n",
      "genre's\n",
      "gentleman's\n",
      "geoffrey's\n",
      "george's\n",
      "gere's\n",
      "germany's\n",
      "get's\n",
      "ghackkk\n",
      "ghost's\n",
      "gi60s\n",
      "gibson's\n",
      "giddons\n",
      "gill's\n",
      "gilliam's\n",
      "gilligan's\n",
      "ginger's\n",
      "girl's\n",
      "girlfriend's\n",
      "glover's\n",
      "go's\n",
      "god's\n",
      "godthumb\n",
      "goethe's\n",
      "gohsts\n",
      "golbahari\n",
      "goldberg's\n",
      "goldblum's\n",
      "goldsmith's\n",
      "goldwyns\n",
      "golem's\n",
      "good.basically\n",
      "goody-little\n",
      "gordon's\n",
      "gore's\n",
      "gosling's\n",
      "grace's\n",
      "grandpa's\n",
      "grant's\n",
      "gray's\n",
      "grayeagle\n",
      "green's\n",
      "greene's\n",
      "greenleaf's\n",
      "greg's\n",
      "gretchen's\n",
      "grey's\n",
      "grier's\n",
      "griffin's\n",
      "griffith's\n",
      "grimbridge\n",
      "grimm's\n",
      "griveous\n",
      "group's\n",
      "grover's\n",
      "gun.this\n",
      "guy's\n",
      "guy.both\n",
      "gwen's\n",
      "h's\n",
      "h.man\n",
      "hackman's\n",
      "hadn't\n",
      "haggard's\n",
      "half's\n",
      "halliwell's\n",
      "hallorans\n",
      "hallström's\n",
      "hamilton's\n",
      "hamlet's\n",
      "hammer's\n",
      "hammerstein's\n",
      "hannibal's\n",
      "hanson's\n",
      "hardy's\n",
      "harm's\n",
      "harriet's\n",
      "harris's\n",
      "harry's\n",
      "harryhausen's\n",
      "hart's\n",
      "hartnett's\n",
      "hartwicke\n",
      "hasn't\n",
      "haven't\n",
      "hawke's\n",
      "hawking's\n",
      "hawn's\n",
      "hayden's\n",
      "hbo's\n",
      "he'd\n",
      "he'll\n",
      "heart's\n",
      "heath's\n",
      "heaven's\n",
      "hedren's\n",
      "heinlein's\n",
      "helgeland's\n",
      "hell's\n",
      "hemingway's\n",
      "henche\n",
      "henry's\n",
      "henson's\n",
      "hepburn's\n",
      "her's\n",
      "here's\n",
      "herman's\n",
      "hero's\n",
      "heroine's\n",
      "hers.the\n",
      "herzog's\n",
      "heston's\n",
      "hill's\n",
      "hillerman's\n",
      "hippie-liberal\n",
      "hitch's\n",
      "hitchcock's\n",
      "hitchhiker's\n",
      "hitler's\n",
      "hitler-type\n",
      "hitperson\n",
      "hoffman's\n",
      "hogan's\n",
      "holden's\n",
      "holger's\n",
      "hollywood's\n",
      "holmesie's\n",
      "homesitter\n",
      "hooper's\n",
      "hopper's\n",
      "hotaru's\n",
      "house's\n",
      "houston's\n",
      "how's\n",
      "howard's\n",
      "howl's\n",
      "hudson's\n",
      "huffman's\n",
      "hulk's\n",
      "humor.it\n",
      "hunter's\n",
      "hurt's\n",
      "husband's\n",
      "huston's\n",
      "hyde's\n",
      "i's\n",
      "ibsen's\n",
      "ice-t's\n",
      "igby's\n",
      "ii.this\n",
      "industry's\n",
      "indy's\n",
      "ingmar's\n",
      "inman's\n",
      "intimacy''s\n",
      "ireland's\n",
      "irene's\n",
      "ironside's\n",
      "irving's\n",
      "island's\n",
      "isn't\n",
      "isshki\n",
      "it'a\n",
      "it'd\n",
      "it'll\n",
      "it--but\n",
      "it--i\n",
      "it--the\n",
      "it.grade\n",
      "jack's\n",
      "jackal's\n",
      "jackie's\n",
      "jackman's\n",
      "jackson's\n",
      "jacob's\n",
      "jacobi's\n",
      "jake's\n",
      "jamie's\n",
      "jane's\n",
      "japan's\n",
      "jason's\n",
      "javutic\n",
      "jay's\n",
      "jean's\n",
      "jeanne's\n",
      "jeff's\n",
      "jekyll's\n",
      "jenkin's\n",
      "jenning's\n",
      "jerry's\n",
      "jessica's\n",
      "jezzie's\n",
      "jim's\n",
      "jimmie's\n",
      "jimmy's\n",
      "jo's\n",
      "joan's\n",
      "joe's\n",
      "joel's\n",
      "john's\n",
      "johnson's\n",
      "joker's\n",
      "jolie's\n",
      "jone's\n",
      "jones's\n",
      "jordan's\n",
      "juan's\n",
      "judd's\n",
      "judy's\n",
      "julia's\n",
      "julie's\n",
      "jump-in-your-chair\n",
      "justine's\n",
      "k's\n",
      "kaempffer\n",
      "kahn's\n",
      "kapplar\n",
      "karloff's\n",
      "katie's\n",
      "kaufman's\n",
      "kazan's\n",
      "kbpsdolby\n",
      "keanu's\n",
      "keaton's\n",
      "kei's\n",
      "keith's\n",
      "kelly's\n",
      "kennedy's\n",
      "kenshin's\n",
      "kerrigan's\n",
      "kessler's\n",
      "kevin's\n",
      "khan's\n",
      "khorda\n",
      "khorda's\n",
      "kid's\n",
      "kidman's\n",
      "kidnap-ransom\n",
      "kieslowski's\n",
      "killer's\n",
      "kilmer's\n",
      "kim's\n",
      "kimble's\n",
      "king's\n",
      "kingsley's\n",
      "kinski's\n",
      "kirk's\n",
      "kit-tay\n",
      "kitano's\n",
      "kitty's\n",
      "knight's\n",
      "knox's\n",
      "konedog\n",
      "kong's\n",
      "kothag\n",
      "kovic's\n",
      "kramer's\n",
      "kristofferson's\n",
      "kubrek\n",
      "kubrick's\n",
      "kurosawa's\n",
      "kurtz's\n",
      "kyle's\n",
      "labute's\n",
      "lacks.the\n",
      "lady's\n",
      "lake's\n",
      "laloux's\n",
      "lambert's\n",
      "lampoon's\n",
      "lane's\n",
      "lang's\n",
      "lanza's\n",
      "larry's\n",
      "latter's\n",
      "laughlin's\n",
      "laughs.the\n",
      "laurie's\n",
      "laverne's\n",
      "law's\n",
      "lawrence's\n",
      "lawyer's\n",
      "lazenby's\n",
      "lean's\n",
      "least.overall\n",
      "leatherface's\n",
      "lechter's\n",
      "ledger's\n",
      "lee's\n",
      "leigh's\n",
      "lemmon's\n",
      "lennon's\n",
      "lenzi's\n",
      "leo's\n",
      "leonard's\n",
      "leone's\n",
      "leopold's\n",
      "leprechaun's\n",
      "lestat's\n",
      "lester's\n",
      "let's\n",
      "levinson's\n",
      "lewis's\n",
      "lie.so\n",
      "life's\n",
      "lilo's\n",
      "lina's\n",
      "lion's\n",
      "lionel's\n",
      "lister's\n",
      "liz's\n",
      "lloyd's\n",
      "lochnar\n",
      "logan's\n",
      "lokai's\n",
      "lopez's\n",
      "lord's\n",
      "lot's\n",
      "lou's\n",
      "love's\n",
      "love-scene\n",
      "lover's\n",
      "lucas's\n",
      "lucy's\n",
      "lugosi's\n",
      "luhrmann's\n",
      "luke's\n",
      "lumet's\n",
      "lundgren's\n",
      "luther's\n",
      "luthor's\n",
      "luzhin's\n",
      "lvwensohn\n",
      "lydeker\n",
      "lydia's\n",
      "lydie-anne\n",
      "lynch's\n",
      "lyne's\n",
      "macanahy\n",
      "macdowell's\n",
      "maclaine's\n",
      "macnee's\n",
      "madea's\n",
      "madonna's\n",
      "magazine's\n",
      "maid's\n",
      "majesty's\n",
      "malick's\n",
      "mallick's\n",
      "malone's\n",
      "malstow\n",
      "malstow's\n",
      "maltin's\n",
      "mama's\n",
      "man's\n",
      "man-show\n",
      "mancini's\n",
      "mankind's\n",
      "mann's\n",
      "manussa\n",
      "manzillo\n",
      "marceau's\n",
      "margaret's\n",
      "mariachi's\n",
      "mariayou\n",
      "marie's\n",
      "mario's\n",
      "mariska's\n",
      "mark's\n",
      "marley's\n",
      "marlin's\n",
      "marlowe's\n",
      "marnau\n",
      "marnau's\n",
      "marrocu\n",
      "marrocu's\n",
      "marshall's\n",
      "martin's\n",
      "marty's\n",
      "mary's\n",
      "massacre.we\n",
      "master's\n",
      "matchchris\n",
      "mathayas\n",
      "matrix's\n",
      "matthau's\n",
      "max's\n",
      "maxford\n",
      "may's\n",
      "mayor's\n",
      "mbpsaverage\n",
      "mcconaughey's\n",
      "mccoy's\n",
      "mcdonald's\n",
      "mcdormand's\n",
      "mcgregor's\n",
      "mcguire's\n",
      "mcqueen's\n",
      "mctierien\n",
      "mcvicar's\n",
      "meaning.in\n",
      "media's\n",
      "megalomanical\n",
      "mel's\n",
      "melkot\n",
      "men's\n",
      "menancing\n",
      "menu's\n",
      "metheor\n",
      "mgm's\n",
      "mia's\n",
      "miami's\n",
      "michael's\n",
      "michelangelo's\n",
      "michele's\n",
      "michelle's\n",
      "mick's\n",
      "might've\n",
      "miike's\n",
      "mike's\n",
      "mikey's\n",
      "miller's\n",
      "mind's\n",
      "mind-screw\n",
      "miner's\n",
      "minutes.rated\n",
      "misbehavers\n",
      "mitch's\n",
      "miyazaki's\n",
      "mo-kei\n",
      "moldiver\n",
      "molina's\n",
      "molinari's\n",
      "molly's\n",
      "mom's\n",
      "moment's\n",
      "momma's\n",
      "money's\n",
      "monster's\n",
      "montag's\n",
      "moon's\n",
      "moonr\n",
      "moore's\n",
      "more's\n",
      "moreau's\n",
      "morley's\n",
      "morricone's\n",
      "morrison's\n",
      "morse's\n",
      "mostow's\n",
      "mother's\n",
      "mouth's\n",
      "movie's\n",
      "movie.all\n",
      "movie.an\n",
      "movie.do\n",
      "movie.first\n",
      "movie.for\n",
      "movie.however\n",
      "movie.now\n",
      "movie.overall\n",
      "movie.some\n",
      "movie.there\n",
      "movie.to\n",
      "movie.when\n",
      "movies.then\n",
      "movies.when\n",
      "movle\n",
      "mozart's\n",
      "mr.spielberg\n",
      "ms.lind\n",
      "mtv's\n",
      "mummy's\n",
      "murdered.the\n",
      "muriel's\n",
      "murnau's\n",
      "murphy's\n",
      "murray's\n",
      "murrow's\n",
      "musical's\n",
      "must've\n",
      "nabeshi\n",
      "nabokov's\n",
      "nadja's\n",
      "nahga\n",
      "nair's\n",
      "name's\n",
      "nancy's\n",
      "napoleon's\n",
      "narraration\n",
      "narrow-screen\n",
      "nation's\n",
      "navin's\n",
      "nazi's\n",
      "nb530mgx\n",
      "nbc's\n",
      "need's\n",
      "neeson's\n",
      "neighbor's\n",
      "neill's\n",
      "nelson's\n",
      "nemo's\n",
      "newman's\n",
      "nicely.there\n",
      "nicholson's\n",
      "nick's\n",
      "niece's\n",
      "nielsen's\n",
      "night's\n",
      "niro's\n",
      "nixon's\n",
      "noah's\n",
      "nobody's\n",
      "noe's\n",
      "nolan's\n",
      "nora's\n",
      "norman's\n",
      "norton's\n",
      "nova's\n",
      "novel's\n",
      "nudity.the\n",
      "nyiszlis\n",
      "o'brien's\n",
      "o'connor's\n",
      "o'donnell's\n",
      "o'flaherty's\n",
      "o'hara's\n",
      "obvious.there\n",
      "ocean's\n",
      "oddyssy\n",
      "odona\n",
      "ofelia's\n",
      "offred's\n",
      "olham\n",
      "olivier's\n",
      "ollie's\n",
      "olsen's\n",
      "one's\n",
      "open-matte\n",
      "opinion.all\n",
      "oprah's\n",
      "original's\n",
      "orwell's\n",
      "oscar.the\n",
      "oseransky\n",
      "osnard\n",
      "other's\n",
      "otld\n",
      "outsider's\n",
      "ozu's\n",
      "ozzy's\n",
      "pacino's\n",
      "pal's\n",
      "pallascio\n",
      "palma's\n",
      "paltrow's\n",
      "pam's\n",
      "pan's\n",
      "paramount's\n",
      "parent's\n",
      "park's\n",
      "parker's\n",
      "parmindar\n",
      "part.what\n",
      "partner's\n",
      "pasolini's\n",
      "pasternak's\n",
      "patriarch's\n",
      "patric's\n",
      "patrick's\n",
      "paul's\n",
      "paula's\n",
      "paxton's\n",
      "pe-tewy\n",
      "peck's\n",
      "peckinpah's\n",
      "pee-wee's\n",
      "peel's\n",
      "peirce's\n",
      "pellenor\n",
      "penn's\n",
      "people's\n",
      "pepper's\n",
      "percy's\n",
      "perdictable\n",
      "persand's\n",
      "person's\n",
      "pesci's\n",
      "pete's\n",
      "peter's\n",
      "petersen's\n",
      "pfeiffer's\n",
      "pharoah's\n",
      "piaf's\n",
      "picard's\n",
      "picture's\n",
      "pie's\n",
      "pig's\n",
      "pignu\n",
      "pirate's\n",
      "pirkanen\n",
      "pitt's\n",
      "pixar's\n",
      "plagerised\n",
      "planet's\n",
      "plasus\n",
      "play's\n",
      "play-ayyy\n",
      "playacts\n",
      "plesence\n",
      "plot's\n",
      "plot--and\n",
      "plot.it\n",
      "polanski's\n",
      "polarisdib\n",
      "polenin\n",
      "polgar's\n",
      "polly's\n",
      "porky's\n",
      "porter's\n",
      "portman's\n",
      "post-matrix\n",
      "post-mtv\n",
      "post-sadam\n",
      "potter's\n",
      "pow's\n",
      "ppv's\n",
      "pradts\n",
      "pratt's\n",
      "preacher's\n",
      "predecessors.the\n",
      "prequils\n",
      "present-girlfriend\n",
      "president's\n",
      "presley's\n",
      "price's\n",
      "prince's\n",
      "princess's\n",
      "princess.the\n",
      "princess.this\n",
      "prodigy's\n",
      "producer's\n",
      "producer-director-writer\n",
      "prof.danforth\n",
      "professor's\n",
      "program's\n",
      "prosecution's\n",
      "protaginists\n",
      "protagonist's\n",
      "protée\n",
      "proulx's\n",
      "pryor's\n",
      "psycharist\n",
      "psychiatrist's\n",
      "public's\n",
      "purlow\n",
      "python's\n",
      "pythoners\n",
      "quantrills\n",
      "quarry's\n",
      "queen's\n",
      "quinlan's\n",
      "quinn's\n",
      "rabbit's\n",
      "raffi's\n",
      "raimi's\n",
      "rambaldi's\n",
      "ramtha's\n",
      "rand's\n",
      "raunchfest\n",
      "ray's\n",
      "reagan's\n",
      "reconsrtuct\n",
      "redford's\n",
      "reed's\n",
      "reeve's\n",
      "regan's\n",
      "reggie's\n",
      "reid's\n",
      "reiner's\n",
      "release.then\n",
      "reno's\n",
      "reuben's\n",
      "reviewer's\n",
      "rewenge\n",
      "rex's\n",
      "riblisi\n",
      "ricci's\n",
      "rice's\n",
      "richard's\n",
      "richardson's\n",
      "ricky's\n",
      "rico's\n",
      "rigg's\n",
      "rimbaud's\n",
      "rimmer's\n",
      "ripley's\n",
      "ritchie's\n",
      "ritter's\n",
      "rival's\n",
      "rivette's\n",
      "robinson's\n",
      "robot's\n",
      "rock's\n",
      "roco's\n",
      "rodriguez's\n",
      "role.and\n",
      "roles.as\n",
      "rollin's\n",
      "romero's\n",
      "room's\n",
      "roommate's\n",
      "rosemary's\n",
      "rossellini's\n",
      "roth's\n",
      "rourke's\n",
      "ruby's\n",
      "ruler's\n",
      "rush's\n",
      "russell's\n",
      "ryan's\n",
      "ryetalyn\n",
      "sagan's\n",
      "sakima's\n",
      "saknussemm's\n",
      "salem's\n",
      "salieri's\n",
      "sam's\n",
      "sandler's\n",
      "sant's\n",
      "sark's\n",
      "sasom\n",
      "satan's\n",
      "saura's\n",
      "sauron's\n",
      "savage's\n",
      "scaramanga's\n",
      "scarlet's\n",
      "sceenes\n",
      "scene's\n",
      "sceneary\n",
      "scenes.it\n",
      "schaeffer's\n",
      "schindler's\n",
      "schlockmeisters\n",
      "schmalzy\n",
      "schmidt's\n",
      "schnitzler's\n",
      "school's\n",
      "schreiber's\n",
      "schumacher's\n",
      "schumann's\n",
      "schwartzeneggar\n",
      "schwarzenegger's\n",
      "sci-fi.the\n",
      "scofield's\n",
      "scorcese's\n",
      "scorsese's\n",
      "scott's\n",
      "screen's\n",
      "screen.on\n",
      "script's\n",
      "seagal's\n",
      "season's\n",
      "seduction.the\n",
      "seem's\n",
      "selena's\n",
      "sellers's\n",
      "senator's\n",
      "sequel.if\n",
      "serial.the\n",
      "series's\n",
      "serra's\n",
      "set's\n",
      "seth's\n",
      "sevrais\n",
      "shakespeare's\n",
      "shame.also\n",
      "shame.in\n",
      "shaq's\n",
      "shark's\n",
      "sharpe's\n",
      "shatner's\n",
      "shaw's\n",
      "she'd\n",
      "she'll\n",
      "sheen's\n",
      "sheffer's\n",
      "shelton's\n",
      "sherman's\n",
      "shin-zon\n",
      "ship's\n",
      "shirley's\n",
      "shoot'em\n",
      "shostakovich's\n",
      "should've\n",
      "should.as\n",
      "shouldn't\n",
      "show's\n",
      "shue's\n",
      "shyamalan's\n",
      "sid's\n",
      "sidekick.the\n",
      "simon's\n",
      "singer's\n",
      "sister's\n",
      "sisto's\n",
      "sky's\n",
      "sky-diver\n",
      "slater's\n",
      "slausen\n",
      "slausen's\n",
      "sleepstalker\n",
      "smith's\n",
      "smokey's\n",
      "so's\n",
      "society's\n",
      "soderbergh's\n",
      "soldier's\n",
      "someone's\n",
      "something's\n",
      "son's\n",
      "sondheim's\n",
      "soneji's\n",
      "soundtrack.the\n",
      "souplier\n",
      "spacey's\n",
      "spade's\n",
      "spartcus\n",
      "speilberg's\n",
      "spider's\n",
      "spielberg's\n",
      "spike's\n",
      "spillane's\n",
      "spinosaurs\n",
      "spock's\n",
      "springsteen's\n",
      "ssintrepid\n",
      "stack's\n",
      "stage-bound\n",
      "stagebound\n",
      "stallone's\n",
      "stan's\n",
      "stanley's\n",
      "stanwyck's\n",
      "stapleton's\n",
      "star's\n",
      "stargates.episode\n",
      "stark's\n",
      "stars.as\n",
      "starskey\n",
      "state's\n",
      "station's\n",
      "std's\n",
      "steed's\n",
      "steel's\n",
      "steenburgen's\n",
      "steiger's\n",
      "steiner's\n",
      "stephen's\n",
      "steve's\n",
      "stevenson's\n",
      "stewart's\n",
      "stick-dog\n",
      "stiller's\n",
      "stng's\n",
      "stoker's\n",
      "stone's\n",
      "story's\n",
      "storyline.the\n",
      "stowe's\n",
      "stranger's\n",
      "stratten's\n",
      "streep's\n",
      "stryker's\n",
      "studio's\n",
      "stupid-funny\n",
      "subtitles.the\n",
      "summer's\n",
      "supe's\n",
      "super-warp\n",
      "superduperman\n",
      "superman's\n",
      "supposed-to-be-alive\n",
      "surprise.this\n",
      "susan's\n",
      "susann's\n",
      "sutherland's\n",
      "svankmajer's\n",
      "swank's\n",
      "swanson's\n",
      "swyne\n",
      "t's\n",
      "takashi's\n",
      "taker's\n",
      "talk.this\n",
      "talsby\n",
      "tamahori's\n",
      "tara's\n",
      "tarantino's\n",
      "tarkovsky's\n",
      "tarzan's\n",
      "taylor's\n",
      "teacher's\n",
      "team's\n",
      "ted's\n",
      "tenchi's\n",
      "tenor's\n",
      "terminator's\n",
      "terriying\n",
      "terrorist's\n",
      "terry's\n",
      "thackeray's\n",
      "that'd\n",
      "that'll\n",
      "there'd\n",
      "there'll\n",
      "theron's\n",
      "they'd\n",
      "they'll\n",
      "they're\n",
      "they've\n",
      "thing's\n",
      "this--and\n",
      "thompson's\n",
      "thor's\n",
      "three's\n",
      "thriller.oh\n",
      "tiffany's\n",
      "tilly's\n",
      "timmonds\n",
      "tita's\n",
      "titlesthis\n",
      "titlesundertaker\n",
      "today's\n",
      "together.no\n",
      "tolkien's\n",
      "tom's\n",
      "tomato's\n",
      "tony's\n",
      "torn's\n",
      "town's\n",
      "townsend's\n",
      "track.overall\n",
      "tracy's\n",
      "trailer.bottom\n",
      "travolta's\n",
      "trek's\n",
      "trevanny\n",
      "trevor's\n",
      "tribe's\n",
      "trier's\n",
      "trinity's\n",
      "troglytes\n",
      "troi's\n",
      "troma's\n",
      "ttwa\n",
      "tudeski\n",
      "turbo-lift\n",
      "turturro's\n",
      "tushiro\n",
      "tv's\n",
      "twain's\n",
      "two's\n",
      "un-remastered\n",
      "unbated\n",
      "uncle's\n",
      "underexplained\n",
      "uninteractive\n",
      "unisols\n",
      "universal's\n",
      "val's\n",
      "valentine's\n",
      "vallier's\n",
      "vampire's\n",
      "vapoorize\n",
      "varnoff\n",
      "veloceraptors\n",
      "verdi's\n",
      "vermeer's\n",
      "vern's\n",
      "verne's\n",
      "vetchy\n",
      "vians\n",
      "vic's\n",
      "victim's\n",
      "victor's\n",
      "videdrome\n",
      "video's\n",
      "viewer's\n",
      "viewer.the\n",
      "viewsrceen\n",
      "village's\n",
      "villain's\n",
      "vince's\n",
      "visconti's\n",
      "visnjic's\n",
      "volk's\n",
      "voltan's\n",
      "volume's\n",
      "vonnegut's\n",
      "voyager's\n",
      "wachowski's\n",
      "walbergh\n",
      "walken's\n",
      "walker's\n",
      "wallace's\n",
      "ward's\n",
      "warden's\n",
      "wargle\n",
      "warhol's\n",
      "warner's\n",
      "warptwist\n",
      "was'nt\n",
      "was.anyone\n",
      "wasen't\n",
      "washington's\n",
      "wasn't\n",
      "watch.on\n",
      "watch.there\n",
      "water's\n",
      "wayne's\n",
      "wcw's\n",
      "we'd\n",
      "we'll\n",
      "we're\n",
      "we've\n",
      "weaver's\n",
      "webber's\n",
      "weber's\n",
      "well's\n",
      "welles's\n",
      "weren't\n",
      "wesley's\n",
      "west's\n",
      "what'd\n",
      "what-can-i-do\n",
      "whedon's\n",
      "when's\n",
      "whendon\n",
      "where's\n",
      "who'd\n",
      "who'll\n",
      "who're\n",
      "who's\n",
      "who've\n",
      "who-she-is-as-a-woman\n",
      "why'd\n",
      "widmark's\n",
      "wife's\n",
      "wilder's\n",
      "wilkensons\n",
      "will's\n",
      "willard's\n",
      "william's\n",
      "williams's\n",
      "willow's\n",
      "wilson's\n",
      "winger's\n",
      "winner's\n",
      "winston's\n",
      "winter's\n",
      "witch's\n",
      "with.even\n",
      "witherspoon's\n",
      "wolf's\n",
      "wolfe's\n",
      "woman's\n",
      "women's\n",
      "won't\n",
      "wong's\n",
      "woo's\n",
      "wood's\n",
      "woodian\n",
      "woody's\n",
      "word's\n",
      "worf's\n",
      "world's\n",
      "world-wide's\n",
      "worstever\n",
      "would'nt\n",
      "would've\n",
      "wouldn't\n",
      "wreacking\n",
      "wreck.the\n",
      "writer's\n",
      "written.one\n",
      "wynter's\n",
      "y'all\n",
      "y'know\n",
      "yazuha\n",
      "year's\n",
      "yee's\n",
      "yi-yi\n",
      "yoeh\n",
      "york's\n",
      "you'd\n",
      "young's\n",
      "yubaba's\n",
      "yulaw's\n",
      "yun-fat's\n",
      "ywasc\n",
      "z-man's\n",
      "zeffirelli's\n",
      "zellweger's\n",
      "zenite\n",
      "zeppelin's\n",
      "zhivago's\n",
      "zimmer's\n",
      "zinnemann's\n",
      "zolov\n",
      "zombie's\n",
      "zsigmond's\n",
      "}:\n",
      "™\n",
      "�\n",
      "Failed to find 1784 out of 30099 tokens.\n"
     ]
    }
   ],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((len(vocab_list), embedding_dim))\n",
    "count = 0\n",
    "for i, word in enumerate(vocab_list):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        count += 1\n",
    "        print(word)\n",
    "print(\"Failed to find {} out of {} tokens.\".format(count, len(vocab_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_s (InputLayer)            (None, 39)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_r (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 39, 300)      9029700     input_s[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 500, 300)     9029700     input_r[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 100)          120300      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 100)          120300      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 200)          0           gru_1[0][0]                      \n",
      "                                                                 gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            402         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 18,300,402\n",
      "Trainable params: 241,002\n",
      "Non-trainable params: 18,059,400\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 7200 samples, validate on 900 samples\n",
      "Epoch 1/100\n",
      "7200/7200 [==============================] - 86s 12ms/step - loss: 0.6019 - acc: 0.6664 - val_loss: 0.4698 - val_acc: 0.7867\n",
      "Epoch 2/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.4937 - acc: 0.7539 - val_loss: 0.4939 - val_acc: 0.7567\n",
      "Epoch 3/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.4342 - acc: 0.7917 - val_loss: 0.3554 - val_acc: 0.8400\n",
      "Epoch 4/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.3973 - acc: 0.8193 - val_loss: 0.3446 - val_acc: 0.8489\n",
      "Epoch 5/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.3624 - acc: 0.8415 - val_loss: 0.3335 - val_acc: 0.8467\n",
      "Epoch 6/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.3391 - acc: 0.8454 - val_loss: 0.2842 - val_acc: 0.8911\n",
      "Epoch 7/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.3082 - acc: 0.8693 - val_loss: 0.2735 - val_acc: 0.8967\n",
      "Epoch 8/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.2930 - acc: 0.8731 - val_loss: 0.2747 - val_acc: 0.8911\n",
      "Epoch 9/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.2700 - acc: 0.8883 - val_loss: 0.2758 - val_acc: 0.8911\n",
      "Epoch 10/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.2618 - acc: 0.8922 - val_loss: 0.2873 - val_acc: 0.8911\n",
      "Epoch 11/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.2527 - acc: 0.8949 - val_loss: 0.2671 - val_acc: 0.8978\n",
      "Epoch 12/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.2440 - acc: 0.8983 - val_loss: 0.2677 - val_acc: 0.8978\n",
      "Epoch 13/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.2351 - acc: 0.9028 - val_loss: 0.2639 - val_acc: 0.8944\n",
      "Epoch 14/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.2307 - acc: 0.9062 - val_loss: 0.2642 - val_acc: 0.8967\n",
      "Epoch 15/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.2148 - acc: 0.9124 - val_loss: 0.3384 - val_acc: 0.8667\n",
      "Epoch 16/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.2004 - acc: 0.9186 - val_loss: 0.2771 - val_acc: 0.8956\n",
      "Epoch 17/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.1974 - acc: 0.9203 - val_loss: 0.2992 - val_acc: 0.8856\n",
      "Epoch 18/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.1828 - acc: 0.9271 - val_loss: 0.3223 - val_acc: 0.8833\n",
      "Epoch 19/100\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.1826 - acc: 0.9289 - val_loss: 0.2780 - val_acc: 0.8944\n",
      "Epoch 00019: early stopping\n",
      "CPU times: user 7min 12s, sys: 25.4 s, total: 7min 37s\n",
      "Wall time: 6min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Best model - Combined \n",
    "\n",
    "# Keras functional API for joined model\n",
    "input_s = Input(shape=(summary_len,), dtype='int32', name='input_s')\n",
    "input_r = Input(shape=(review_len,), dtype='int32', name='input_r')\n",
    "\n",
    "embedding_vector_length = embedding_dim\n",
    "GRU_nodes_summary = 100\n",
    "GRU_nodes_review = 100\n",
    "\n",
    "emb_s = Embedding(len(vocab_list), embedding_vector_length, mask_zero=True,\n",
    "                  input_length=summary_len, weights=[embedding_matrix], trainable=False)(input_s)\n",
    "emb_r = Embedding(len(vocab_list), embedding_vector_length, mask_zero=True,\n",
    "                  input_length=review_len, weights=[embedding_matrix], trainable=False)(input_r)\n",
    "\n",
    "gru_s = GRU(GRU_nodes_summary, activation='tanh', recurrent_activation='sigmoid', dropout=0.4, \n",
    "              recurrent_dropout=0.3, kernel_constraint=maxnorm(4), recurrent_constraint=maxnorm(5),\n",
    "              unroll=True, \n",
    "            \n",
    "              use_bias=True, kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "              kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, \n",
    "              activity_regularizer=None,  \n",
    "              bias_constraint=None, implementation=1, return_sequences=False, return_state=False, \n",
    "              go_backwards=False, stateful=False, reset_after=False)(emb_s)\n",
    "gru_r = GRU(GRU_nodes_review, activation='tanh', recurrent_activation='sigmoid', dropout=0.4, \n",
    "              recurrent_dropout=0.3, unroll=True, \n",
    "              \n",
    "              kernel_constraint=None, recurrent_constraint=None,\n",
    "              use_bias=True, kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "              kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, \n",
    "              activity_regularizer=None,  \n",
    "              bias_constraint=None, implementation=1, return_sequences=False, return_state=False, \n",
    "              go_backwards=False, stateful=False, reset_after=False)(emb_r)\n",
    "\n",
    "concat = Concatenate()([gru_s, gru_r])\n",
    "#calc = Dense(GRU_nodes_summary+GRU_nodes_review+3, activation='relu')(concat) # this might be superfluous?\n",
    "output = Dense(len(label_set), activation='softmax')(concat)\n",
    "model = Model([input_s, input_r], output)\n",
    "nadam = keras.optimizers.nadam(lr=0.0008)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6)\n",
    "hist = model.fit(x=[train_summary, train_review], \n",
    "                 y=y_train, \n",
    "                 validation_data=([validation_summary, validation_review], \n",
    "                                  y_validation), \n",
    "                 epochs=100, batch_size=128, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses=hist.history['loss']\n",
    "accuracies=hist.history['acc']\n",
    "print(\"Training loss / accuracy\")\n",
    "plot_results(losses,accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=hist.history['val_loss']\n",
    "accuracies=hist.history['val_acc']\n",
    "print(\"Validation loss / accuracy\")\n",
    "plot_results(losses,accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for validation data \n",
    "y_pred = model.predict([validation_summary, validation_review])\n",
    "\n",
    "# Undo one-hot\n",
    "y_pred = undo_one_hot(y_pred, label_list)\n",
    "y_orig = validation['polarity']\n",
    "\n",
    "print(\"Validation data, confusion\")\n",
    "plot_confusion(y_orig, y_pred, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Keras functional API for joined model w/ metadata\n",
    "input_s = Input(shape=(summary_len,), dtype='int32', name='input_s')\n",
    "input_r = Input(shape=(review_len,), dtype='int32', name='input_r')\n",
    "input_wc_s = Input(shape=(1,), dtype='float32', name='input_wc_s')\n",
    "input_wc_r = Input(shape=(1,), dtype='float32', name='input_wc_r')\n",
    "input_year = Input(shape=(1,), dtype='float32', name='input_year')\n",
    "\n",
    "embedding_vector_length = embedding_dim\n",
    "GRU_nodes_summary = 100\n",
    "GRU_nodes_review = 100\n",
    "\n",
    "emb_s = Embedding(len(vocab_list), embedding_vector_length, mask_zero=True,\n",
    "                  input_length=summary_len, weights=[embedding_matrix], trainable=False)(input_s)\n",
    "emb_r = Embedding(len(vocab_list), embedding_vector_length, mask_zero=True,\n",
    "                  input_length=review_len, weights=[embedding_matrix], trainable=False)(input_r)\n",
    "\n",
    "gru_s = GRU(GRU_nodes_summary, activation='relu', recurrent_activation='sigmoid', dropout=0.3, \n",
    "              recurrent_dropout=0.3, kernel_constraint=maxnorm(4), recurrent_constraint=maxnorm(5),\n",
    "              unroll=True, \n",
    "            \n",
    "              use_bias=True, kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "              kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, \n",
    "              activity_regularizer=None,  \n",
    "              bias_constraint=None, implementation=1, return_sequences=False, return_state=False, \n",
    "              go_backwards=False, stateful=False, reset_after=False)(emb_s)\n",
    "gru_r = GRU(GRU_nodes_review, activation='relu', recurrent_activation='sigmoid', dropout=0.3, \n",
    "              recurrent_dropout=0.3, unroll=True, \n",
    "              \n",
    "              kernel_constraint=None, recurrent_constraint=None,\n",
    "              use_bias=True, kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "              kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, \n",
    "              activity_regularizer=None,  \n",
    "              bias_constraint=None, implementation=1, return_sequences=False, return_state=False, \n",
    "              go_backwards=False, stateful=False, reset_after=False)(emb_r)\n",
    "\n",
    "concat1 = Concatenate()([input_wc_s, input_wc_r, input_year])\n",
    "calc = Dense(32, activation='relu')(concat1) \n",
    "\n",
    "concat2 = Concatenate()([gru_s, gru_r, calc])\n",
    "\n",
    "output = Dense(len(label_set), activation='softmax')(concat2)\n",
    "model = Model([input_s, input_r, input_wc_s, input_wc_r, input_year], output)\n",
    "nadam = keras.optimizers.nadam(lr=0.0006)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6)\n",
    "hist = model.fit(x=[train_summary, train_review, train['summary_wc'], train['review_wc'], train['year']], \n",
    "                 y=y_train, \n",
    "                 validation_data=([validation_summary, validation_review, \n",
    "                                   validation['summary_wc'], validation['review_wc'],\n",
    "                                   validation['year']], \n",
    "                                  y_validation), \n",
    "                 epochs=60, batch_size=128, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses=hist.history['loss']\n",
    "accuracies=hist.history['acc']\n",
    "print(\"Training loss / accuracy\")\n",
    "plot_results(losses,accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=hist.history['val_loss']\n",
    "accuracies=hist.history['val_acc']\n",
    "print(\"Validation loss / accuracy\")\n",
    "plot_results(losses,accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for validation data \n",
    "y_pred = model.predict([validation_summary, validation_review, \n",
    "                        validation['summary_wc'], validation['review_wc'],\n",
    "                        validation['year']])\n",
    "\n",
    "# Undo one-hot\n",
    "y_pred = undo_one_hot(y_pred, label_list)\n",
    "y_orig = validation['polarity']\n",
    "\n",
    "print(\"Validation data, confusion\")\n",
    "plot_confusion(y_orig, y_pred, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight in model.get_weights():\n",
    "    df = pd.DataFrame(weight)\n",
    "    print(df[df.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove embeddings by removing \"weights=[embedding_matrix]\" from Keras code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# create and run the model (with summaries)\n",
    "embedding_vector_length = embedding_dim\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocab_list), embedding_vector_length, mask_zero=True,\n",
    "                    input_length=summary_len, weights=[embedding_matrix], trainable=False))\n",
    "model.add(GRU(100, activation='relu', recurrent_activation='sigmoid', dropout=0.3, \n",
    "              recurrent_dropout=0.3, kernel_constraint=maxnorm(4), recurrent_constraint=maxnorm(5),\n",
    "\n",
    "              use_bias=True, kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "              kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, \n",
    "              activity_regularizer=None,  \n",
    "              bias_constraint=None, implementation=1, return_sequences=False, return_state=False, \n",
    "              go_backwards=False, stateful=False, unroll=True, reset_after=False))\n",
    "model.add(Dense(len(label_set), activation='softmax'))\n",
    "nadam = keras.optimizers.nadam(lr=0.0006)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6)\n",
    "hist = model.fit(x=train_summary, y=y_train, \n",
    "                 validation_data=(validation_summary, y_validation), \n",
    "                 epochs=50, batch_size=128, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses=hist.history['loss']\n",
    "accuracies=hist.history['acc']\n",
    "print(\"Training loss / accuracy\")\n",
    "plot_results(losses,accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=hist.history['val_loss']\n",
    "accuracies=hist.history['val_acc']\n",
    "print(\"Validation loss / accuracy\")\n",
    "plot_results(losses,accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for validation data \n",
    "y_pred = model.predict(validation_summary)\n",
    "\n",
    "# Undo one-hot\n",
    "y_pred = undo_one_hot(y_pred, label_list)\n",
    "y_orig = validation['polarity']\n",
    "\n",
    "print(\"Validation data, confusion\")\n",
    "plot_confusion(y_orig, y_pred, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight in model.get_weights():\n",
    "    df = pd.DataFrame(weight)\n",
    "    print(df[df.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# create and run the model (with reviews)\n",
    "embedding_vector_length = embedding_dim\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocab_list), embedding_vector_length, mask_zero=True,\n",
    "                    input_length=review_len, weights=[embedding_matrix], trainable=False))\n",
    "model.add(GRU(100, activation='relu', recurrent_activation='sigmoid', dropout=0.3, \n",
    "              recurrent_dropout=0.3, \n",
    "              \n",
    "              kernel_constraint=None, recurrent_constraint=None,\n",
    "              use_bias=True, kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "              kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, \n",
    "              activity_regularizer=None,  \n",
    "              bias_constraint=None, implementation=1, return_sequences=False, return_state=False, \n",
    "              go_backwards=False, stateful=False, unroll=True, reset_after=False))\n",
    "model.add(Dense(len(label_set), activation='softmax'))\n",
    "nadam = keras.optimizers.nadam(lr=0.0006, clipvalue=0.5)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6)\n",
    "hist = model.fit(x=train_review, y=y_train, \n",
    "                 validation_data=(validation_review, y_validation), \n",
    "                 epochs=50, batch_size=128, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses=hist.history['loss']\n",
    "accuracies=hist.history['acc']\n",
    "print(\"Training loss / accuracy\")\n",
    "plot_results(losses,accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses=hist.history['val_loss']\n",
    "accuracies=hist.history['val_acc']\n",
    "print(\"Validation loss / accuracy\")\n",
    "plot_results(losses,accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight in model.get_weights():\n",
    "    df = pd.DataFrame(weight)\n",
    "    print(df[df.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for validation data \n",
    "y_pred = model.predict(validation_review)\n",
    "\n",
    "# Undo one-hot\n",
    "y_pred = undo_one_hot(y_pred, label_list)\n",
    "y_orig = validation['polarity']\n",
    "\n",
    "print(\"Validation data, confusion\")\n",
    "plot_confusion(y_orig, y_pred, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
