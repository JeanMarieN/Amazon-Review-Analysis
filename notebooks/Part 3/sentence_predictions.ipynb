{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# external libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from ast import literal_eval\n",
    "import time\n",
    "import sys \n",
    "from shutil import copyfile\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# tensorflow and keras\n",
    "import keras.optimizers\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Concatenate, Bidirectional, Reshape\n",
    "from keras.layers import GRU, CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.constraints import maxnorm\n",
    "from keras.regularizers import L1L2\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.backend import tile\n",
    "import keras.backend as K\n",
    "from keras.layers import Lambda\n",
    "# fix random seed for reproducibility - only works for CPU version of tensorflow\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = pd.read_csv('../../../../data/processed/tok_sentence_baby_reviews_spell.csv')\n",
    "reviews_df = pd.read_csv('../../../../data/processed/tok_baby_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reviews_df.merge(sentences_df, on='uuid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Files read, converting tokens to lists.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFiles read, converting tokens to lists.\")\n",
    "for col in ['summary_tokens', 'review_tokens', 'sentence_tokens']:\n",
    "    df[col] = df[col].map(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>polarity</th>\n",
       "      <th>year</th>\n",
       "      <th>uuid</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>summary_tokens</th>\n",
       "      <th>summary_wc</th>\n",
       "      <th>review_wc</th>\n",
       "      <th>summary_wc_std</th>\n",
       "      <th>review_wc_std</th>\n",
       "      <th>year_std</th>\n",
       "      <th>group_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>sentence_wc</th>\n",
       "      <th>sentence_wc_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>2005</td>\n",
       "      <td>1114646400B000056OUGA3FFDK09UJS1TD</td>\n",
       "      <td>B000056OUG</td>\n",
       "      <td>A3FFDK09UJS1TD</td>\n",
       "      <td>[i, am, primarily, breastfeeding, ,, but, i, t...</td>\n",
       "      <td>[infrequent, bottle, user, ?, then, buy, steam...</td>\n",
       "      <td>10</td>\n",
       "      <td>84</td>\n",
       "      <td>1.560512</td>\n",
       "      <td>-0.494464</td>\n",
       "      <td>-0.845298</td>\n",
       "      <td>1</td>\n",
       "      <td>neg</td>\n",
       "      <td>[i, am, primarily, breastfeeding, but, i, thou...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.249451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>2005</td>\n",
       "      <td>1114646400B000056OUGA3FFDK09UJS1TD</td>\n",
       "      <td>B000056OUG</td>\n",
       "      <td>A3FFDK09UJS1TD</td>\n",
       "      <td>[i, am, primarily, breastfeeding, ,, but, i, t...</td>\n",
       "      <td>[infrequent, bottle, user, ?, then, buy, steam...</td>\n",
       "      <td>10</td>\n",
       "      <td>84</td>\n",
       "      <td>1.560512</td>\n",
       "      <td>-0.494464</td>\n",
       "      <td>-0.845298</td>\n",
       "      <td>1</td>\n",
       "      <td>neu</td>\n",
       "      <td>[i, discovered, medela, microwave, steam, clea...</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.118286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>2005</td>\n",
       "      <td>1114646400B000056OUGA3FFDK09UJS1TD</td>\n",
       "      <td>B000056OUG</td>\n",
       "      <td>A3FFDK09UJS1TD</td>\n",
       "      <td>[i, am, primarily, breastfeeding, ,, but, i, t...</td>\n",
       "      <td>[infrequent, bottle, user, ?, then, buy, steam...</td>\n",
       "      <td>10</td>\n",
       "      <td>84</td>\n",
       "      <td>1.560512</td>\n",
       "      <td>-0.494464</td>\n",
       "      <td>-0.845298</td>\n",
       "      <td>1</td>\n",
       "      <td>neu</td>\n",
       "      <td>[i, feel, badly, for, the, waste, of, money, b...</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.118286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>2005</td>\n",
       "      <td>1114646400B000056OUGA3FFDK09UJS1TD</td>\n",
       "      <td>B000056OUG</td>\n",
       "      <td>A3FFDK09UJS1TD</td>\n",
       "      <td>[i, am, primarily, breastfeeding, ,, but, i, t...</td>\n",
       "      <td>[infrequent, bottle, user, ?, then, buy, steam...</td>\n",
       "      <td>10</td>\n",
       "      <td>84</td>\n",
       "      <td>1.560512</td>\n",
       "      <td>-0.494464</td>\n",
       "      <td>-0.845298</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[and, not, space, consuming, .]</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.129561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>2005</td>\n",
       "      <td>1114646400B000056OUGA3FFDK09UJS1TD</td>\n",
       "      <td>B000056OUG</td>\n",
       "      <td>A3FFDK09UJS1TD</td>\n",
       "      <td>[i, am, primarily, breastfeeding, ,, but, i, t...</td>\n",
       "      <td>[infrequent, bottle, user, ?, then, buy, steam...</td>\n",
       "      <td>10</td>\n",
       "      <td>84</td>\n",
       "      <td>1.560512</td>\n",
       "      <td>-0.494464</td>\n",
       "      <td>-0.845298</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "      <td>[plus, you, can, use, them, during, travel, at...</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.577956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  polarity  year                                uuid        asin  \\\n",
       "0           0  negative  2005  1114646400B000056OUGA3FFDK09UJS1TD  B000056OUG   \n",
       "1           0  negative  2005  1114646400B000056OUGA3FFDK09UJS1TD  B000056OUG   \n",
       "2           0  negative  2005  1114646400B000056OUGA3FFDK09UJS1TD  B000056OUG   \n",
       "3           0  negative  2005  1114646400B000056OUGA3FFDK09UJS1TD  B000056OUG   \n",
       "4           0  negative  2005  1114646400B000056OUGA3FFDK09UJS1TD  B000056OUG   \n",
       "\n",
       "       reviewerID                                      review_tokens  \\\n",
       "0  A3FFDK09UJS1TD  [i, am, primarily, breastfeeding, ,, but, i, t...   \n",
       "1  A3FFDK09UJS1TD  [i, am, primarily, breastfeeding, ,, but, i, t...   \n",
       "2  A3FFDK09UJS1TD  [i, am, primarily, breastfeeding, ,, but, i, t...   \n",
       "3  A3FFDK09UJS1TD  [i, am, primarily, breastfeeding, ,, but, i, t...   \n",
       "4  A3FFDK09UJS1TD  [i, am, primarily, breastfeeding, ,, but, i, t...   \n",
       "\n",
       "                                      summary_tokens  summary_wc  review_wc  \\\n",
       "0  [infrequent, bottle, user, ?, then, buy, steam...          10         84   \n",
       "1  [infrequent, bottle, user, ?, then, buy, steam...          10         84   \n",
       "2  [infrequent, bottle, user, ?, then, buy, steam...          10         84   \n",
       "3  [infrequent, bottle, user, ?, then, buy, steam...          10         84   \n",
       "4  [infrequent, bottle, user, ?, then, buy, steam...          10         84   \n",
       "\n",
       "   summary_wc_std  review_wc_std  year_std  group_id sentiment  \\\n",
       "0        1.560512      -0.494464 -0.845298         1       neg   \n",
       "1        1.560512      -0.494464 -0.845298         1       neu   \n",
       "2        1.560512      -0.494464 -0.845298         1       neu   \n",
       "3        1.560512      -0.494464 -0.845298         1       pos   \n",
       "4        1.560512      -0.494464 -0.845298         1       pos   \n",
       "\n",
       "                                     sentence_tokens  sentence_wc  \\\n",
       "0  [i, am, primarily, breastfeeding, but, i, thou...           20   \n",
       "1  [i, discovered, medela, microwave, steam, clea...           16   \n",
       "2  [i, feel, badly, for, the, waste, of, money, b...           16   \n",
       "3                    [and, not, space, consuming, .]            5   \n",
       "4  [plus, you, can, use, them, during, travel, at...           11   \n",
       "\n",
       "   sentence_wc_std  \n",
       "0         0.249451  \n",
       "1        -0.118286  \n",
       "2        -0.118286  \n",
       "3        -1.129561  \n",
       "4        -0.577956  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9429 tokens before discarding those that appear less than 2 times.\n",
      "5313 tokens after discarding those that appear less than 2 times.\n"
     ]
    }
   ],
   "source": [
    "### Preprocessing \n",
    "# declare the padding and unknown symbols\n",
    "pad_mask_int = 0\n",
    "pad_mask_sym = '==pad_mask=='\n",
    "unknown_int = 1\n",
    "unknown_sym = '==unknown_sym=='\n",
    "\n",
    "# vocabulary set\n",
    "vocab_counter = Counter()\n",
    "for doc in df['sentence_tokens']:\n",
    "    vocab_counter.update(doc)\n",
    "\n",
    "min_times_word_used = 2 # if at least 2 then the model will be prepared for unknown words in test and validation sets\n",
    "print(len(vocab_counter), \"tokens before discarding those that appear less than {} times.\".format(min_times_word_used))\n",
    "for key in list(vocab_counter.keys()):\n",
    "    if vocab_counter[key] < min_times_word_used: \n",
    "        vocab_counter.pop(key)\n",
    "print(len(vocab_counter), \"tokens after discarding those that appear less than {} times.\".format(min_times_word_used))   \n",
    "vocab_set = set(vocab_counter.keys())\n",
    "\n",
    "# vocabulary list and int map\n",
    "vocab_list = [pad_mask_sym, unknown_sym] + sorted(vocab_set)\n",
    "vocab_map = {word: index for index, word in enumerate(vocab_list)}\n",
    "\n",
    "# label set\n",
    "label_set = set(df['sentiment'].unique())\n",
    "\n",
    "# label list and int map\n",
    "label_list = sorted(label_set)\n",
    "label_map = {word: index for index, word in enumerate(label_list)}\n",
    "\n",
    "# polarity feature set\n",
    "polarity_set = set(df['polarity'].unique())\n",
    "\n",
    "# polarity list and int map\n",
    "polarity_list = sorted(polarity_set)\n",
    "polarity_map = {word: index for index, word in enumerate(polarity_list)}\n",
    "\n",
    "# group feature set\n",
    "group_set = set(df['group_id'].unique())\n",
    "\n",
    "# group list and int map\n",
    "group_list = sorted(group_set)\n",
    "group_map = {word: index for index, word in enumerate(group_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading big ol' word embeddings\n",
      "Loaded 1193514 word vectors.\n",
      "==pad_mask==\n",
      "==unknown_sym==\n",
      "(8\n",
      "):\n",
      ");\n",
      "->\n",
      "..\n",
      "0\n",
      "00\n",
      "06\n",
      "07\n",
      "1\n",
      "1-2\n",
      "1/2\n",
      "1/3\n",
      "1/4\n",
      "10\n",
      "10-12\n",
      "10-15\n",
      "100\n",
      "10oz\n",
      "11\n",
      "12\n",
      "12-18\n",
      "120\n",
      "13\n",
      "130\n",
      "14\n",
      "15\n",
      "150\n",
      "159\n",
      "16\n",
      "17\n",
      "18\n",
      "18mo\n",
      "19\n",
      "1st\n",
      "1yo\n",
      "2\n",
      "2-1\n",
      "2-3\n",
      "20\n",
      "200\n",
      "2002\n",
      "2004\n",
      "2005\n",
      "2007\n",
      "2009\n",
      "2012\n",
      "21\n",
      "22\n",
      "22lbs\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "29\n",
      "2nd\n",
      "2yo\n",
      "2yr\n",
      "3\n",
      "3-4\n",
      "3/4\n",
      "30\n",
      "30-45\n",
      "300\n",
      "32\n",
      "34\n",
      "35\n",
      "37\n",
      "39\n",
      "3m\n",
      "3rd\n",
      "3yo\n",
      "4\n",
      "4-5\n",
      "4-6\n",
      "40\n",
      "45\n",
      "4oz\n",
      "4th\n",
      "5\n",
      "5-10\n",
      "50\n",
      "500\n",
      "6\n",
      "6-7\n",
      "600\n",
      "6mo\n",
      "7\n",
      "7-8\n",
      "70\n",
      "75\n",
      "77\n",
      "7oz\n",
      "8\n",
      "8-9\n",
      "80\n",
      "8lbs\n",
      "8oz\n",
      "9\n",
      "90\n",
      "900\n",
      "900mhz\n",
      "99\n",
      "9oz\n",
      ":(\n",
      ":)\n",
      ":-)\n",
      "::\n",
      ":P\n",
      ";)\n",
      ";-)\n",
      "=)\n",
      "adiri\n",
      "ameda\n",
      "armholes\n",
      "attatch\n",
      "avents\n",
      "babiesrus\n",
      "baby-proofing\n",
      "babyhawk\n",
      "barely-used\n",
      "baseboards\n",
      "basinett\n",
      "born-free\n",
      "bouncenette\n",
      "breast-fed\n",
      "breastpump\n",
      "breastshield\n",
      "breastshields\n",
      "brown's\n",
      "car-seats\n",
      "cd's\n",
      "childproof\n",
      "colapsed\n",
      "colicky\n",
      "compactly\n",
      "cosleeper\n",
      "cosleeping\n",
      "cozyup\n",
      "cushie\n",
      "doens't\n",
      "dreamscapes\n",
      "drop-ins\n",
      "dropins\n",
      "engorged\n",
      "engorgement\n",
      "eurobath\n",
      "evenflow\n",
      "excema\n",
      "exersaucer\n",
      "flammability\n",
      "fraying\n",
      "freedislike\n",
      "freshners\n",
      "front-to-back\n",
      "furnature\n",
      "fussier\n",
      "fussiness\n",
      "heatable\n",
      "highchairs\n",
      "hygeia\n",
      "i'm\n",
      "i've\n",
      "indentations\n",
      "inverts\n",
      "iq24\n",
      "itzbeen\n",
      "jumperoo\n",
      "keyfit\n",
      "kiddopotamus\n",
      "kneeler\n",
      "kooshies\n",
      "lactina\n",
      "linkadoos\n",
      "medella\n",
      "microfleece\n",
      "middle-of-the-night\n",
      "milkmate\n",
      "months-old\n",
      "no-slip\n",
      "non-adjustable\n",
      "nurser\n",
      "nursers\n",
      "orignally\n",
      "pack-n-play\n",
      "platex\n",
      "playyard\n",
      "playyards\n",
      "positioner\n",
      "positioners\n",
      "pottys\n",
      "pre-shrunk\n",
      "prefolds\n",
      "prewashed\n",
      "pump-in-style\n",
      "pupsqueak\n",
      "rear-facing\n",
      "recevied\n",
      "recommed\n",
      "releive\n",
      "removeable\n",
      "safeseats\n",
      "santoprene\n",
      "seatback\n",
      "side-to-side\n",
      "sleepsacks\n",
      "slow-flow\n",
      "snackies\n",
      "snoogle\n",
      "snowsuits\n",
      "snuggli\n",
      "snugli\n",
      "snuzzler\n",
      "soothie\n",
      "soothies\n",
      "spigots\n",
      "spitup\n",
      "squirmer\n",
      "sterilizers\n",
      "sterilizes\n",
      "sterilizing\n",
      "sterlize\n",
      "sterlizer\n",
      "stinkier\n",
      "storkcraft\n",
      "sturdier\n",
      "suctioned\n",
      "suctioning\n",
      "suctions\n",
      "superyard\n",
      "swaddleme\n",
      "swaddler\n",
      "swaddles\n",
      "taggies\n",
      "teethers\n",
      "terrycloth\n",
      "tri-flow\n",
      "twist-tie\n",
      "twistaway\n",
      "unpadded\n",
      "unsnap\n",
      "unswaddled\n",
      "uppababy\n",
      "ventair\n",
      "ventaire\n",
      "ventaires\n",
      "volumne\n",
      "washables\n",
      "washings\n",
      "woombie\n",
      "wriggly\n",
      "you-shape\n",
      "255\n"
     ]
    }
   ],
   "source": [
    "# pretrained embeddings are from https://nlp.stanford.edu/projects/glove/\n",
    "# start by loading in the embedding matrix\n",
    "# load the whole embedding into memory\n",
    "print(\"\\nReading big ol' word embeddings\")\n",
    "count = 0\n",
    "embeddings_index_1 = dict()\n",
    "with open('../../../../data/external/glove.twitter.27B.50d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            print(values)\n",
    "        embeddings_index_1[word] = coefs\n",
    "print('Loaded %s word vectors.' % len(embeddings_index_1))\n",
    "\n",
    "#embeddings_index_2 = dict()\n",
    "#with open('../../../data/external/glove.twitter.27B.100d.txt') as f:\n",
    "#    for line in f:\n",
    "#        values = line.split()\n",
    "#        word = values[0]\n",
    "#        try:\n",
    "#            coefs = np.asarray(values[1:], dtype='float32')\n",
    "#        except:\n",
    "#            print(values)\n",
    "#        embeddings_index_2[word] = coefs\n",
    "#print('Loaded %s word vectors.' % len(embeddings_index_2))\n",
    "\n",
    "embedding_dim_1 = 50\n",
    "embedding_dim_2 = 0\n",
    "\n",
    "embedding_dim = embedding_dim_1 + embedding_dim_2\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "if embedding_dim_2 > 0:\n",
    "    embedding_matrix = np.zeros((len(vocab_list), embedding_dim))\n",
    "    for i, word in enumerate(vocab_list):\n",
    "        embedding_vector_1 = embeddings_index_1.get(word)\n",
    "        embedding_vector_2 = embeddings_index_2.get(word)\n",
    "        if embedding_vector_1 is not None and embedding_vector_2 is not None:\n",
    "            embedding_matrix[i] = np.concatenate((embedding_vector_1, embedding_vector_2))\n",
    "        elif embedding_vector_1 is None and embedding_vector_2 is not None:\n",
    "            embedding_matrix[i] = np.concatenate((np.zeros(embedding_dim_1), embedding_vector_2))        \n",
    "        elif embedding_vector_1 is not None and embedding_vector_2 is None:\n",
    "            embedding_matrix[i] = np.concatenate((embedding_vector_1, np.zeros(embedding_dim_2)))\n",
    "        else:\n",
    "            print(word)\n",
    "            count += 1 # maybe we should use fuzzywuzzy to get vector of nearest word? Instead of all zeros\n",
    "else:\n",
    "    embedding_matrix = np.zeros((len(vocab_list), embedding_dim))\n",
    "    for i, word in enumerate(vocab_list):\n",
    "        embedding_vector = embeddings_index_1.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            print(word)\n",
    "            count += 1 # maybe we should use fuzzywuzzy to get vector of nearest word? Instead of all zeros\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse \n",
    "from typing import List, Set, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "\n",
    "def create_one_hot(labels, label_dict: dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        labels:        array of labels, e.g. NumPy array or Pandas Series\n",
    "        label_dict:    dict of label indices\n",
    "    Return:\n",
    "        one_hot_numpy: sparse CSR 2d array of one-hot vectors\n",
    "    \"\"\"\n",
    "    one_hot_numpy = sparse.dok_matrix((len(labels), len(label_dict)), dtype=np.int8)\n",
    "    for i, label in enumerate(labels):\n",
    "        one_hot_numpy[i, label_dict[label]] = 1\n",
    "    return sparse.csr_matrix(one_hot_numpy) \n",
    "\n",
    "def undo_one_hot(pred, label_list: list) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    \n",
    "    Args: \n",
    "        pred:       NumPy array of one-hot predicted classes\n",
    "        label_list: a list of the label strings\n",
    "    Return:\n",
    "        label_pred: a list of predicted labels\n",
    "    \"\"\"\n",
    "    label_pred = [label_list[np.argmax(row)] for row in pred]\n",
    "    return label_pred\n",
    "    # this could probably be done awesomely fast as NumPy vectorised but it works\n",
    "\n",
    "\n",
    "def word_index(los: List[List[str]], vocab_dict: Dict[str, int], unknown: int, reverse: bool=False) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Replaces words with integers from a vocabulary dictionary or else with the integer for unknown\n",
    "    \n",
    "    Args:\n",
    "        los:     list of lists of split sentences\n",
    "        pad_to:  how big to make the padded list\n",
    "        unknown: the integer to put in for unknown tokens (either because they were pruned or not seen in training set)\n",
    "        reverse: reverse the order of tokens in the sub-list \n",
    "    Returns: \n",
    "        new_los: list of lists of split sentences where each token is replaced by an integer\n",
    "        \n",
    "    Examples:\n",
    "    >>> print(word_index([['one', 'two', 'three'], ['one', 'two']], {'one': 1, 'two': 2, 'three': 3}, unknown=4))\n",
    "    [[1, 2, 3], [1, 2]]\n",
    "    >>> print(word_index([['one', 'two', 'three'], ['one', 'two']], {'one': 1, 'two': 2, 'three': 3}, unknown=4, reverse=True))\n",
    "    [[3, 2, 1], [2, 1]]\n",
    "    \"\"\"\n",
    "    new_los = []\n",
    "    if reverse:\n",
    "        for sentence in los:\n",
    "            new_los.append([vocab_dict[word] if word in vocab_dict else unknown for word in sentence][::-1])        \n",
    "    else:\n",
    "        for sentence in los:\n",
    "            new_los.append([vocab_dict[word] if word in vocab_dict else unknown for word in sentence])\n",
    "    return new_los\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one-hot sparse matrix of labels\n",
    "y = create_one_hot(df['sentiment'], label_map).todense()\n",
    "\n",
    "# create one-hot of review polarity\n",
    "polarity = create_one_hot(df['polarity'], polarity_map)[:, 0].todense()\n",
    "\n",
    "# create one-hot of group number\n",
    "group = create_one_hot(df['group_id'], group_map).todense()\n",
    "\n",
    "\n",
    "# replace strings with ints (tokenization is done on the Series fed to word_index())\n",
    "sentences = word_index(df['sentence_tokens'], vocab_map, unknown_int, reverse=False)\n",
    "\n",
    "# pad / truncate \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentence_len = max(map(len, list(df['sentence_tokens'])))\n",
    "\n",
    "sentences = pad_sequences(sequences=sentences, \n",
    "                              maxlen=sentence_len, \n",
    "                              dtype='int32', \n",
    "                              padding='pre', \n",
    "                              value=pad_mask_int)\n",
    "\n",
    "#group = pad_sequences(sequences=group, \n",
    "#                              maxlen=embedding_dim, \n",
    "#                              dtype='int32', \n",
    "#                              padding='pre', \n",
    "#                              value=pad_mask_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0 2268\n",
      "   281 3490  665  750 2268 4718 4712 5260  493 4998 1852 4419 3546 3249\n",
      "   295 4682 3079  629   20]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0 2268 1356 2804 2830 4404  916  443 2368  295 4712 4704\n",
      "  5167 2302 4682  942   20]]\n",
      "[[1]\n",
      " [1]]\n",
      "[[1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]]\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:2])\n",
    "print(polarity[:2])\n",
    "print(group[:2])\n",
    "print(y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_s (InputLayer)         (None, 159)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 159, 50)           265750    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 8)                 1416      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 267,193\n",
      "Trainable params: 1,443\n",
      "Non-trainable params: 265,750\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 15654 samples, validate on 1749 samples\n",
      "Epoch 1/50\n",
      "15654/15654 [==============================] - 47s 3ms/step - loss: 1.0935 - acc: 0.4026 - val_loss: 1.0712 - val_acc: 0.4197\n",
      "Epoch 2/50\n",
      "15654/15654 [==============================] - 45s 3ms/step - loss: 1.0424 - acc: 0.4524 - val_loss: 1.0406 - val_acc: 0.4391\n",
      "Epoch 3/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 1.0146 - acc: 0.4813 - val_loss: 1.0146 - val_acc: 0.4780\n",
      "Epoch 4/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.9882 - acc: 0.5015 - val_loss: 1.0089 - val_acc: 0.4774\n",
      "Epoch 5/50\n",
      "15654/15654 [==============================] - 45s 3ms/step - loss: 0.9729 - acc: 0.5206 - val_loss: 0.9741 - val_acc: 0.5163\n",
      "Epoch 6/50\n",
      "15654/15654 [==============================] - 45s 3ms/step - loss: 0.9653 - acc: 0.5242 - val_loss: 0.9742 - val_acc: 0.5152\n",
      "Epoch 7/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.9559 - acc: 0.5323 - val_loss: 0.9624 - val_acc: 0.5272\n",
      "Epoch 8/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.9471 - acc: 0.5403 - val_loss: 0.9512 - val_acc: 0.5294\n",
      "Epoch 9/50\n",
      "15654/15654 [==============================] - 42s 3ms/step - loss: 0.9453 - acc: 0.5446 - val_loss: 0.9556 - val_acc: 0.5294\n",
      "Epoch 10/50\n",
      "15654/15654 [==============================] - 42s 3ms/step - loss: 0.9382 - acc: 0.5454 - val_loss: 0.9507 - val_acc: 0.5300\n",
      "Epoch 11/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.9317 - acc: 0.5500 - val_loss: 0.9433 - val_acc: 0.5312\n",
      "Epoch 12/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.9276 - acc: 0.5540 - val_loss: 0.9434 - val_acc: 0.5472\n",
      "Epoch 13/50\n",
      "15654/15654 [==============================] - 42s 3ms/step - loss: 0.9214 - acc: 0.5597 - val_loss: 0.9232 - val_acc: 0.5609\n",
      "Epoch 14/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.9168 - acc: 0.5625 - val_loss: 0.9245 - val_acc: 0.5586\n",
      "Epoch 15/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.9066 - acc: 0.5685 - val_loss: 0.9031 - val_acc: 0.5746\n",
      "Epoch 16/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.9081 - acc: 0.5690 - val_loss: 0.8980 - val_acc: 0.5780\n",
      "Epoch 17/50\n",
      "15654/15654 [==============================] - 42s 3ms/step - loss: 0.9048 - acc: 0.5707 - val_loss: 0.9101 - val_acc: 0.5678\n",
      "Epoch 18/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.8986 - acc: 0.5758 - val_loss: 0.8928 - val_acc: 0.5820\n",
      "Epoch 19/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8967 - acc: 0.5744 - val_loss: 0.8975 - val_acc: 0.5803\n",
      "Epoch 20/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.8901 - acc: 0.5787 - val_loss: 0.8788 - val_acc: 0.5878\n",
      "Epoch 21/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.8894 - acc: 0.5807 - val_loss: 0.8765 - val_acc: 0.5872\n",
      "Epoch 22/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.8860 - acc: 0.5790 - val_loss: 0.8721 - val_acc: 0.5923\n",
      "Epoch 23/50\n",
      "15654/15654 [==============================] - 40s 3ms/step - loss: 0.8857 - acc: 0.5769 - val_loss: 0.8663 - val_acc: 0.5952\n",
      "Epoch 24/50\n",
      "15654/15654 [==============================] - 40s 3ms/step - loss: 0.8834 - acc: 0.5831 - val_loss: 0.8654 - val_acc: 0.5941\n",
      "Epoch 25/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.8819 - acc: 0.5829 - val_loss: 0.8668 - val_acc: 0.5958\n",
      "Epoch 26/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.8806 - acc: 0.5834 - val_loss: 0.8658 - val_acc: 0.5923\n",
      "Epoch 27/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.8764 - acc: 0.5895 - val_loss: 0.8626 - val_acc: 0.5918\n",
      "Epoch 28/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.8766 - acc: 0.5869 - val_loss: 0.8583 - val_acc: 0.5969\n",
      "Epoch 29/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.8738 - acc: 0.5851 - val_loss: 0.8598 - val_acc: 0.5946\n",
      "Epoch 30/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.8757 - acc: 0.5877 - val_loss: 0.8546 - val_acc: 0.5946\n",
      "Epoch 31/50\n",
      "15654/15654 [==============================] - 40s 3ms/step - loss: 0.8709 - acc: 0.5898 - val_loss: 0.8491 - val_acc: 0.6049\n",
      "Epoch 32/50\n",
      "15654/15654 [==============================] - 45s 3ms/step - loss: 0.8691 - acc: 0.5885 - val_loss: 0.8594 - val_acc: 0.5963\n",
      "Epoch 33/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.8679 - acc: 0.5903 - val_loss: 0.8496 - val_acc: 0.6043\n",
      "Epoch 34/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.8675 - acc: 0.5921 - val_loss: 0.8399 - val_acc: 0.6198\n",
      "Epoch 35/50\n",
      "15654/15654 [==============================] - 49s 3ms/step - loss: 0.8680 - acc: 0.5918 - val_loss: 0.8485 - val_acc: 0.6061\n",
      "Epoch 36/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8684 - acc: 0.5867 - val_loss: 0.8409 - val_acc: 0.6089\n",
      "Epoch 37/50\n",
      "15654/15654 [==============================] - 45s 3ms/step - loss: 0.8643 - acc: 0.5954 - val_loss: 0.8434 - val_acc: 0.6049\n",
      "Epoch 38/50\n",
      "15654/15654 [==============================] - 45s 3ms/step - loss: 0.8662 - acc: 0.5928 - val_loss: 0.8390 - val_acc: 0.6129\n",
      "Epoch 39/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.8630 - acc: 0.5896 - val_loss: 0.8424 - val_acc: 0.6066\n",
      "Epoch 40/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.8632 - acc: 0.5924 - val_loss: 0.8355 - val_acc: 0.6204\n",
      "Epoch 41/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8650 - acc: 0.5928 - val_loss: 0.8425 - val_acc: 0.6118\n",
      "Epoch 42/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.8587 - acc: 0.5982 - val_loss: 0.8365 - val_acc: 0.6164\n",
      "Epoch 43/50\n",
      "15654/15654 [==============================] - 40s 3ms/step - loss: 0.8593 - acc: 0.5936 - val_loss: 0.8307 - val_acc: 0.6192\n",
      "Epoch 44/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.8606 - acc: 0.5960 - val_loss: 0.8384 - val_acc: 0.6106\n",
      "Epoch 45/50\n",
      "15654/15654 [==============================] - 40s 3ms/step - loss: 0.8587 - acc: 0.5994 - val_loss: 0.8399 - val_acc: 0.6146\n",
      "Epoch 46/50\n",
      "15654/15654 [==============================] - 40s 3ms/step - loss: 0.8597 - acc: 0.5964 - val_loss: 0.8371 - val_acc: 0.6118\n",
      "Epoch 47/50\n",
      "15654/15654 [==============================] - 40s 3ms/step - loss: 0.8590 - acc: 0.5955 - val_loss: 0.8314 - val_acc: 0.6164\n",
      "Epoch 48/50\n",
      "15654/15654 [==============================] - 40s 3ms/step - loss: 0.8593 - acc: 0.5932 - val_loss: 0.8393 - val_acc: 0.6135\n",
      "Epoch 49/50\n",
      "15654/15654 [==============================] - 40s 3ms/step - loss: 0.8573 - acc: 0.5986 - val_loss: 0.8308 - val_acc: 0.6221\n",
      "Epoch 50/50\n",
      "15654/15654 [==============================] - 40s 3ms/step - loss: 0.8586 - acc: 0.5977 - val_loss: 0.8343 - val_acc: 0.6169\n",
      "\n",
      "    Group 6\n",
      "     Sklearn\n",
      "      f1 micro 0.616923956546598\n",
      "      f1 macro is 0.6015291191221407\n",
      "      Accuracy 0.616923956546598\n",
      "     TF\n",
      "      ['val_loss 0.8343095490426728', 'val_acc 0.6169239567169944', 'loss 0.8586453056981224', 'acc 0.5977385970951269']\n",
      "      \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_s (InputLayer)         (None, 159)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 159, 50)           265750    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 8)                 1416      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 267,193\n",
      "Trainable params: 1,443\n",
      "Non-trainable params: 265,750\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15654 samples, validate on 1773 samples\n",
      "Epoch 1/50\n",
      "15654/15654 [==============================] - 42s 3ms/step - loss: 1.0737 - acc: 0.4209 - val_loss: 0.9947 - val_acc: 0.5290\n",
      "Epoch 2/50\n",
      "15654/15654 [==============================] - 40s 3ms/step - loss: 1.0315 - acc: 0.4691 - val_loss: 0.9372 - val_acc: 0.5668\n",
      "Epoch 3/50\n",
      "15654/15654 [==============================] - 40s 3ms/step - loss: 1.0011 - acc: 0.4992 - val_loss: 0.9085 - val_acc: 0.5883\n",
      "Epoch 4/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.9756 - acc: 0.5196 - val_loss: 0.8807 - val_acc: 0.6148\n",
      "Epoch 5/50\n",
      "15654/15654 [==============================] - 40s 3ms/step - loss: 0.9635 - acc: 0.5257 - val_loss: 0.8720 - val_acc: 0.6131\n",
      "Epoch 6/50\n",
      "15654/15654 [==============================] - 40s 3ms/step - loss: 0.9566 - acc: 0.5337 - val_loss: 0.8682 - val_acc: 0.6170\n",
      "Epoch 7/50\n",
      "15654/15654 [==============================] - 40s 3ms/step - loss: 0.9496 - acc: 0.5394 - val_loss: 0.8518 - val_acc: 0.6328\n",
      "Epoch 8/50\n",
      "15654/15654 [==============================] - 50s 3ms/step - loss: 0.9444 - acc: 0.5459 - val_loss: 0.8524 - val_acc: 0.6306\n",
      "Epoch 9/50\n",
      "15654/15654 [==============================] - 50s 3ms/step - loss: 0.9373 - acc: 0.5468 - val_loss: 0.8537 - val_acc: 0.6356\n",
      "Epoch 10/50\n",
      "15654/15654 [==============================] - 49s 3ms/step - loss: 0.9348 - acc: 0.5508 - val_loss: 0.8393 - val_acc: 0.6464\n",
      "Epoch 11/50\n",
      "15654/15654 [==============================] - 51s 3ms/step - loss: 0.9287 - acc: 0.5544 - val_loss: 0.8371 - val_acc: 0.6413\n",
      "Epoch 12/50\n",
      "15654/15654 [==============================] - 55s 3ms/step - loss: 0.9287 - acc: 0.5562 - val_loss: 0.8307 - val_acc: 0.6469\n",
      "Epoch 13/50\n",
      "15654/15654 [==============================] - 52s 3ms/step - loss: 0.9186 - acc: 0.5586 - val_loss: 0.8379 - val_acc: 0.6402\n",
      "Epoch 14/50\n",
      "15654/15654 [==============================] - 46s 3ms/step - loss: 0.9166 - acc: 0.5659 - val_loss: 0.8358 - val_acc: 0.6447\n",
      "Epoch 15/50\n",
      "15654/15654 [==============================] - 52s 3ms/step - loss: 0.9145 - acc: 0.5662 - val_loss: 0.8361 - val_acc: 0.6492\n",
      "Epoch 16/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.9112 - acc: 0.5672 - val_loss: 0.8273 - val_acc: 0.6548\n",
      "Epoch 17/50\n",
      "15654/15654 [==============================] - 45s 3ms/step - loss: 0.9063 - acc: 0.5731 - val_loss: 0.8140 - val_acc: 0.6582\n",
      "Epoch 18/50\n",
      "15654/15654 [==============================] - 45s 3ms/step - loss: 0.9028 - acc: 0.5744 - val_loss: 0.8156 - val_acc: 0.6503\n",
      "Epoch 19/50\n",
      "15654/15654 [==============================] - 45s 3ms/step - loss: 0.8986 - acc: 0.5749 - val_loss: 0.8226 - val_acc: 0.6543\n",
      "Epoch 20/50\n",
      "15654/15654 [==============================] - 46s 3ms/step - loss: 0.9005 - acc: 0.5714 - val_loss: 0.8108 - val_acc: 0.6576\n",
      "Epoch 21/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8942 - acc: 0.5779 - val_loss: 0.8010 - val_acc: 0.6672\n",
      "Epoch 22/50\n",
      "15654/15654 [==============================] - 51s 3ms/step - loss: 0.8953 - acc: 0.5723 - val_loss: 0.8009 - val_acc: 0.6695\n",
      "Epoch 23/50\n",
      "15654/15654 [==============================] - 58s 4ms/step - loss: 0.8925 - acc: 0.5760 - val_loss: 0.7998 - val_acc: 0.6661\n",
      "Epoch 24/50\n",
      "15654/15654 [==============================] - 56s 4ms/step - loss: 0.8908 - acc: 0.5721 - val_loss: 0.7993 - val_acc: 0.6599\n",
      "Epoch 25/50\n",
      "15654/15654 [==============================] - 49s 3ms/step - loss: 0.8907 - acc: 0.5789 - val_loss: 0.8040 - val_acc: 0.6616\n",
      "Epoch 26/50\n",
      "15654/15654 [==============================] - 46s 3ms/step - loss: 0.8850 - acc: 0.5810 - val_loss: 0.7940 - val_acc: 0.6678\n",
      "Epoch 27/50\n",
      "15654/15654 [==============================] - 42s 3ms/step - loss: 0.8820 - acc: 0.5841 - val_loss: 0.7854 - val_acc: 0.6638\n",
      "Epoch 28/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.8798 - acc: 0.5853 - val_loss: 0.7861 - val_acc: 0.6633\n",
      "Epoch 29/50\n",
      "15654/15654 [==============================] - 42s 3ms/step - loss: 0.8785 - acc: 0.5914 - val_loss: 0.7922 - val_acc: 0.6701\n",
      "Epoch 30/50\n",
      "15654/15654 [==============================] - 42s 3ms/step - loss: 0.8823 - acc: 0.5847 - val_loss: 0.7795 - val_acc: 0.6672\n",
      "Epoch 31/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.8763 - acc: 0.5845 - val_loss: 0.7793 - val_acc: 0.6723\n",
      "Epoch 32/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.8766 - acc: 0.5875 - val_loss: 0.7812 - val_acc: 0.6723\n",
      "Epoch 33/50\n",
      "15654/15654 [==============================] - 41s 3ms/step - loss: 0.8741 - acc: 0.5918 - val_loss: 0.7722 - val_acc: 0.6717\n",
      "Epoch 34/50\n",
      "15654/15654 [==============================] - 47s 3ms/step - loss: 0.8739 - acc: 0.5885 - val_loss: 0.7862 - val_acc: 0.6712\n",
      "Epoch 35/50\n",
      "15654/15654 [==============================] - 50s 3ms/step - loss: 0.8710 - acc: 0.5886 - val_loss: 0.7878 - val_acc: 0.6695\n",
      "Epoch 36/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8728 - acc: 0.5925 - val_loss: 0.7770 - val_acc: 0.6779\n",
      "Epoch 37/50\n",
      "15654/15654 [==============================] - 48s 3ms/step - loss: 0.8716 - acc: 0.5859 - val_loss: 0.7738 - val_acc: 0.6717\n",
      "Epoch 38/50\n",
      "15654/15654 [==============================] - 49s 3ms/step - loss: 0.8716 - acc: 0.5894 - val_loss: 0.7836 - val_acc: 0.6678\n",
      "Epoch 39/50\n",
      "15654/15654 [==============================] - 46s 3ms/step - loss: 0.8690 - acc: 0.5922 - val_loss: 0.7736 - val_acc: 0.6717\n",
      "Epoch 40/50\n",
      "15654/15654 [==============================] - 46s 3ms/step - loss: 0.8683 - acc: 0.5939 - val_loss: 0.7795 - val_acc: 0.6706\n",
      "Epoch 41/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8635 - acc: 0.5921 - val_loss: 0.7686 - val_acc: 0.6734\n",
      "Epoch 42/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8667 - acc: 0.5885 - val_loss: 0.7736 - val_acc: 0.6717\n",
      "Epoch 43/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8669 - acc: 0.5965 - val_loss: 0.7697 - val_acc: 0.6763\n",
      "Epoch 44/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8671 - acc: 0.5873 - val_loss: 0.7768 - val_acc: 0.6706\n",
      "Epoch 45/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8647 - acc: 0.5951 - val_loss: 0.7636 - val_acc: 0.6706\n",
      "Epoch 46/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.8611 - acc: 0.5947 - val_loss: 0.7711 - val_acc: 0.6678\n",
      "Epoch 47/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.8608 - acc: 0.5949 - val_loss: 0.7631 - val_acc: 0.6695\n",
      "Epoch 48/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8632 - acc: 0.5914 - val_loss: 0.7789 - val_acc: 0.6661\n",
      "Epoch 49/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8614 - acc: 0.5969 - val_loss: 0.7664 - val_acc: 0.6740\n",
      "Epoch 50/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8616 - acc: 0.5942 - val_loss: 0.7723 - val_acc: 0.6717\n",
      "\n",
      "    Group 7\n",
      "     Sklearn\n",
      "      f1 micro 0.6717428087986463\n",
      "      f1 macro is 0.6020591806324684\n",
      "      Accuracy 0.6717428087986463\n",
      "     TF\n",
      "      ['val_loss 0.772277628844294', 'val_acc 0.671742809168444', 'loss 0.8615684076730541', 'acc 0.5942251181197347']\n",
      "      \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_s (InputLayer)         (None, 159)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 159, 50)           265750    \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 8)                 1416      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 267,193\n",
      "Trainable params: 1,443\n",
      "Non-trainable params: 265,750\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 15654 samples, validate on 1663 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15654/15654 [==============================] - 46s 3ms/step - loss: 1.1082 - acc: 0.3878 - val_loss: 1.2187 - val_acc: 0.1810\n",
      "Epoch 2/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 1.0487 - acc: 0.4481 - val_loss: 1.2476 - val_acc: 0.2147\n",
      "Epoch 3/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 1.0289 - acc: 0.4718 - val_loss: 1.1760 - val_acc: 0.2345\n",
      "Epoch 4/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 1.0068 - acc: 0.4946 - val_loss: 1.1566 - val_acc: 0.2586\n",
      "Epoch 5/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.9868 - acc: 0.5123 - val_loss: 1.1887 - val_acc: 0.2664\n",
      "Epoch 6/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.9673 - acc: 0.5255 - val_loss: 1.1010 - val_acc: 0.3295\n",
      "Epoch 7/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.9547 - acc: 0.5341 - val_loss: 1.1082 - val_acc: 0.3337\n",
      "Epoch 8/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.9448 - acc: 0.5408 - val_loss: 1.1021 - val_acc: 0.3379\n",
      "Epoch 9/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.9332 - acc: 0.5484 - val_loss: 1.0553 - val_acc: 0.4155\n",
      "Epoch 10/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.9257 - acc: 0.5527 - val_loss: 1.0444 - val_acc: 0.4336\n",
      "Epoch 11/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.9208 - acc: 0.5561 - val_loss: 1.0680 - val_acc: 0.3903\n",
      "Epoch 12/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.9174 - acc: 0.5603 - val_loss: 1.0905 - val_acc: 0.3951\n",
      "Epoch 13/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.9142 - acc: 0.5613 - val_loss: 1.0670 - val_acc: 0.4119\n",
      "Epoch 14/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.9079 - acc: 0.5672 - val_loss: 1.0283 - val_acc: 0.4522\n",
      "Epoch 15/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.9057 - acc: 0.5634 - val_loss: 1.0374 - val_acc: 0.4125\n",
      "Epoch 16/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.9092 - acc: 0.5668 - val_loss: 1.0435 - val_acc: 0.4330\n",
      "Epoch 17/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.9012 - acc: 0.5689 - val_loss: 1.0409 - val_acc: 0.4227\n",
      "Epoch 18/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.9015 - acc: 0.5711 - val_loss: 1.0611 - val_acc: 0.4324\n",
      "Epoch 19/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.8956 - acc: 0.5725 - val_loss: 1.0453 - val_acc: 0.4390\n",
      "Epoch 20/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8959 - acc: 0.5749 - val_loss: 1.0250 - val_acc: 0.4576\n",
      "Epoch 21/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8896 - acc: 0.5733 - val_loss: 1.0464 - val_acc: 0.4462\n",
      "Epoch 22/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.8878 - acc: 0.5789 - val_loss: 1.0365 - val_acc: 0.4432\n",
      "Epoch 23/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.8867 - acc: 0.5797 - val_loss: 1.0262 - val_acc: 0.4498\n",
      "Epoch 24/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8876 - acc: 0.5776 - val_loss: 1.0564 - val_acc: 0.4432\n",
      "Epoch 25/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8829 - acc: 0.5803 - val_loss: 1.0121 - val_acc: 0.4672\n",
      "Epoch 26/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.8858 - acc: 0.5810 - val_loss: 0.9995 - val_acc: 0.4660\n",
      "Epoch 27/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8826 - acc: 0.5813 - val_loss: 0.9996 - val_acc: 0.4853\n",
      "Epoch 28/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8810 - acc: 0.5807 - val_loss: 0.9668 - val_acc: 0.4985\n",
      "Epoch 29/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8854 - acc: 0.5751 - val_loss: 1.0262 - val_acc: 0.4492\n",
      "Epoch 30/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8763 - acc: 0.5807 - val_loss: 0.9497 - val_acc: 0.5213\n",
      "Epoch 31/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.8777 - acc: 0.5847 - val_loss: 1.0167 - val_acc: 0.4678\n",
      "Epoch 32/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.8733 - acc: 0.5871 - val_loss: 1.0179 - val_acc: 0.4756\n",
      "Epoch 33/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.8739 - acc: 0.5850 - val_loss: 1.0087 - val_acc: 0.4714\n",
      "Epoch 34/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.8732 - acc: 0.5859 - val_loss: 0.9982 - val_acc: 0.4877\n",
      "Epoch 35/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8691 - acc: 0.5858 - val_loss: 1.0406 - val_acc: 0.4378\n",
      "Epoch 36/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8680 - acc: 0.5892 - val_loss: 1.0041 - val_acc: 0.4720\n",
      "Epoch 37/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8707 - acc: 0.5892 - val_loss: 0.9936 - val_acc: 0.4907\n",
      "Epoch 38/50\n",
      "15654/15654 [==============================] - 45s 3ms/step - loss: 0.8689 - acc: 0.5903 - val_loss: 0.9962 - val_acc: 0.4762\n",
      "Epoch 39/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8696 - acc: 0.5867 - val_loss: 1.0323 - val_acc: 0.4708\n",
      "Epoch 40/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.8669 - acc: 0.5873 - val_loss: 1.0367 - val_acc: 0.4540\n",
      "Epoch 41/50\n",
      "15654/15654 [==============================] - 46s 3ms/step - loss: 0.8667 - acc: 0.5880 - val_loss: 0.9994 - val_acc: 0.4576\n",
      "Epoch 42/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8656 - acc: 0.5906 - val_loss: 0.9661 - val_acc: 0.5177\n",
      "Epoch 43/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8648 - acc: 0.5891 - val_loss: 0.9627 - val_acc: 0.4895\n",
      "Epoch 44/50\n",
      "15654/15654 [==============================] - 43s 3ms/step - loss: 0.8628 - acc: 0.5932 - val_loss: 0.9483 - val_acc: 0.5111\n",
      "Epoch 45/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8592 - acc: 0.5898 - val_loss: 0.9553 - val_acc: 0.5346\n",
      "Epoch 46/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8598 - acc: 0.5919 - val_loss: 1.0172 - val_acc: 0.4624\n",
      "Epoch 47/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8552 - acc: 0.5977 - val_loss: 1.0319 - val_acc: 0.4666\n",
      "Epoch 48/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8575 - acc: 0.5939 - val_loss: 0.9784 - val_acc: 0.5147\n",
      "Epoch 49/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8605 - acc: 0.5940 - val_loss: 0.9754 - val_acc: 0.4865\n",
      "Epoch 50/50\n",
      "15654/15654 [==============================] - 44s 3ms/step - loss: 0.8585 - acc: 0.5958 - val_loss: 0.9840 - val_acc: 0.4781\n",
      "\n",
      "    Group 8\n",
      "     Sklearn\n",
      "      f1 micro 0.47805171377029465\n",
      "      f1 macro is 0.4817129306466874\n",
      "      Accuracy 0.47805171377029465\n",
      "     TF\n",
      "      ['val_loss 0.9839780348578778', 'val_acc 0.4780517146663356', 'loss 0.8585387152761613', 'acc 0.5958221540211017']\n",
      "      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./tb_logs/sentences-ablation-group-9-190517_1842.ipynb'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAME = 'sentences-ablation-group-9-{}'.format(time.strftime('%y%m%d_%H%M', time.localtime(time.time())))\n",
    "\n",
    "for g in range(6,9):\n",
    "    training_mask = np.logical_or(df['group_id'] != g, df['group_id'] != 9)  \n",
    "    validation_mask = df['group_id'] == g  \n",
    "        \n",
    "    input_s = Input(shape=(sentence_len,), dtype='int32', name='input_s')\n",
    "    input_p = Input(shape=(1,), dtype='float32', name='input_p')\n",
    "    input_g = Input(shape=(len(group_list),), dtype='float32', name='input_g')\n",
    "\n",
    "    embedding_vector_length = embedding_dim\n",
    "    GRU_nodes_sentences = 8\n",
    "\n",
    "    emb = Embedding(len(vocab_list), embedding_vector_length, mask_zero=True,\n",
    "                        weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "    emb_s = emb(input_s)\n",
    "\n",
    "    gru_s = GRU(GRU_nodes_sentences,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            recurrent_initializer='orthogonal',\n",
    "            bias_initializer='zeros',\n",
    "            kernel_regularizer=None,\n",
    "            recurrent_regularizer=None,\n",
    "            bias_regularizer=L1L2(l1=0.1, l2=0.0),\n",
    "            activity_regularizer=L1L2(l1=1e-07, l2=0.0),\n",
    "            kernel_constraint=maxnorm(3),\n",
    "            recurrent_constraint=maxnorm(3),\n",
    "            bias_constraint=None,\n",
    "            return_sequences=False,\n",
    "            return_state=False,\n",
    "            go_backwards=False,\n",
    "            stateful=False,\n",
    "            dropout=0.3)(emb_s)\n",
    "\n",
    "    concat_1 = Concatenate()([gru_s,  input_p, input_g]) # \n",
    "    output = Dense(len(label_set), activation='softmax')(gru_s)\n",
    "    model = Model([input_s, input_p, input_g], output) # , ,  \n",
    "    nadam = keras.optimizers.nadam(lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    tensorboard = TensorBoard(log_dir = './tb_logs/{}'.format('group_'+str(g)+'_'+NAME))\n",
    "\n",
    "    hist1 = model.fit(x=[sentences[training_mask], polarity[training_mask], group[training_mask]], #  , \n",
    "                      y=y[training_mask], \n",
    "                      validation_data=([sentences[validation_mask],\n",
    "                                        polarity[validation_mask],\n",
    "                                        group[validation_mask]], #   \n",
    "                                       y[validation_mask]), \n",
    "                      epochs=50, batch_size=64, callbacks=[tensorboard]) \n",
    "    pred = model.predict([sentences[validation_mask], \n",
    "                          polarity[validation_mask], \n",
    "                          group[validation_mask]]) #  \n",
    "    pred = undo_one_hot(pred, label_list)\n",
    "    true_sentiment = df.loc[validation_mask, 'sentiment']\n",
    "    \n",
    "    f1_micro = f1_score(true_sentiment, pred, average='micro')\n",
    "    f1_macro = f1_score(true_sentiment, pred, average='macro')\n",
    "    accu = accuracy_score(true_sentiment, pred)\n",
    "\n",
    "    metrics_string = \"\"\"\n",
    "    Group {}\n",
    "     Sklearn\n",
    "      f1 micro {}\n",
    "      f1 macro is {}\n",
    "      Accuracy {}\n",
    "     TF\n",
    "      {}\n",
    "      \"\"\".format(g, f1_micro, f1_macro, accu, [key + \" \" + str(hist1.history[key][-1])  for key in hist1.history.keys()])\n",
    "    print(metrics_string)\n",
    "    \n",
    "    with open(NAME+'.txt', mode='a') as fp:\n",
    "        fp.write(metrics_string)\n",
    "\n",
    "copyfile('sentence_predictions.ipynb', './tb_logs/{}.ipynb'.format(NAME)) # sys.argv[0] for .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(pred, true_sentiment, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sentiment = df.loc[np.logical_not(group_mask), 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = undo_one_hot(pred, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['val_loss 0.7653999250259758',\n",
       " 'val_acc 0.6525821599042472',\n",
       " 'loss 0.7189908383824065',\n",
       " 'acc 0.6974193549498008']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key + \" \" + str(hist1.history[key][-1])  for key in hist1.history.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
